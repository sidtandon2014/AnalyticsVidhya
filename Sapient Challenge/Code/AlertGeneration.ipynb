{"nbformat_minor": 2, "cells": [{"source": "## Start Spark session", "cell_type": "markdown", "metadata": {}}, {"execution_count": 153, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1531460095132_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:8088/proxy/application_1531460095132_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sidspa.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:30060/node/containerlogs/container_1531460095132_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1531460095132_0010</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:8088/proxy/application_1531460095132_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sidspa.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:30060/node/containerlogs/container_1531460095132_0010_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 154, "cell_type": "code", "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.streaming.Trigger", "outputs": [{"output_type": "stream", "name": "stdout", "text": "import org.apache.spark.sql.streaming.Trigger"}], "metadata": {"collapsed": false}}, {"source": "## Drop topics", "cell_type": "markdown", "metadata": {}}, {"execution_count": 155, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic rawevents --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic streamPerHourPerDay --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic streamPerHour --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic streamPerHourAlert2 --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Topic rawevents is marked for deletion.\nNote: This will have no impact if delete.topic.enable is not set to true.\nTopic streamPerHourPerDay is marked for deletion.\nNote: This will have no impact if delete.topic.enable is not set to true.\nTopic streamPerHour is marked for deletion.\nNote: This will have no impact if delete.topic.enable is not set to true.\nTopic streamPerHourAlert2 is marked for deletion.\nNote: This will have no impact if delete.topic.enable is not set to true.\n"}], "metadata": {"collapsed": false}}, {"source": "## Create topics", "cell_type": "markdown", "metadata": {}}, {"execution_count": 156, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 1 --partitions 3 --topic rawevents --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 1 --partitions 3 --topic streamPerHourPerDay --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 1 --partitions 3 --topic streamPerHour --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 1 --partitions 3 --topic streamPerHourAlert2 --zookeeper \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Created topic \"rawevents\".\nCreated topic \"streamPerHourPerDay\".\nCreated topic \"streamPerHour\".\nCreated topic \"streamPerHourAlert2\".\n"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "## Delete all checkpoints directory and output folders", "cell_type": "markdown", "metadata": {}}, {"execution_count": 157, "cell_type": "code", "source": "%%bash\nhdfs dfs -rm -r /path/checkpointstreamPerHour\nhdfs dfs -rm -r /path/checkpointstreamPerHourPerDay\nhdfs dfs -rm -r /path/chkpointalertType1query\nhdfs dfs -rm -r /path/checkpointstreamPerHourAlert2\nhdfs dfs -rm -r /path/chkpointalertType2queryNew\nhdfs dfs -rm -r /example/AlertType1\nhdfs dfs -rm -r /example/AlertType2", "outputs": [{"output_type": "stream", "name": "stderr", "text": "18/07/15 07:22:00 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/15 07:25:50 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 230670 ms with threads: 0\n18/07/15 07:25:51 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/path/checkpointstreamPerHour' to trash at: wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/path/checkpointstreamPerHour1531639312534\nrm: `/path/checkpointstreamPerHourPerDay': No such file or directory\n18/07/15 07:26:08 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/15 07:30:36 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 267772 ms with threads: 0\n18/07/15 07:30:36 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/path/chkpointalertType1query' to trash at: wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/path/chkpointalertType1query1531639556091\n18/07/15 07:30:47 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/15 07:34:50 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 242743 ms with threads: 0\n18/07/15 07:34:50 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/path/checkpointstreamPerHourAlert2' to trash at: wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/path/checkpointstreamPerHourAlert21531639839432\n18/07/15 07:35:10 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/15 07:39:14 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 244859 ms with threads: 0\n18/07/15 07:39:15 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/path/chkpointalertType2queryNew' to trash at: wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/path/chkpointalertType2queryNew1531640093042\n18/07/15 07:39:18 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/15 07:39:19 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 830 ms with threads: 0\n18/07/15 07:39:19 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/example/AlertType1' to trash at: wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/example/AlertType11531640358201\n18/07/15 07:39:23 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/15 07:39:23 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 702 ms with threads: 0\n18/07/15 07:39:23 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/example/AlertType2' to trash at: wasb://sidsparkcluster-2018-07-13t05-21-51-393z@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/example/AlertType21531640362979\n"}], "metadata": {"collapsed": false}}, {"source": "## Create schemas", "cell_type": "markdown", "metadata": {}}, {"execution_count": 144, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"wn0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:9092,wn1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:9092,wn2-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:9092\"\nval kafkaTopic=\"rawevents\"\nval zookeepers = \"zk0-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk1-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181,zk3-kafka.vehan4uacj1epaynq40fn1fpke.cx.internal.cloudapp.net:2181\"\n\n// Define a schema for the data\nval schema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"eventtimestamp\", StringType).\nadd(\"value\", StringType).\nadd(\"household_id\", StringType).\nadd(\"timestamp\",StringType)\n\nvar kafkastreamPerHourPerDaySchema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"household_id\", StringType).\nadd(\"Date\", DateType).\nadd(\"LastHour\", StringType).\nadd(\"Id\", StringType).\nadd(\"TotalConsumptionForThatHourAndDay\",DoubleType).\nadd(\"timestamp\",IntegerType).\nadd(\"window\",(new StructType).\n    add(\"start\",TimestampType).\n    add(\"end\",TimestampType)\n   )\n\nvar kafkastreamPerHourSchema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"household_id\", StringType).\nadd(\"LastHour\", StringType).\nadd(\"MeanConsumptionForThatHour\",DoubleType).\nadd(\"SDConsumptionForThatHour\",DoubleType).\nadd(\"1SDConsumption\", DoubleType).\nadd(\"timestamp\",IntegerType).\nadd(\"window\",(new StructType).\n    add(\"start\",TimestampType).\n    add(\"end\",TimestampType)\n   )\n\nvar kafkastreamPerHourSchemaAlert2 = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"LastHour\", StringType).\nadd(\"MeanConsumptionForThatHour\",DoubleType).\nadd(\"SDConsumptionForThatHour\",DoubleType).\nadd(\"1SDConsumption\", DoubleType).\nadd(\"timestamp\",IntegerType).\nadd(\"window\",(new StructType).\n    add(\"start\",TimestampType).\n    add(\"end\",TimestampType)\n   )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "kafkastreamPerHourSchemaAlert2: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,StringType,true), StructField(LastHour,StringType,true), StructField(MeanConsumptionForThatHour,DoubleType,true), StructField(SDConsumptionForThatHour,DoubleType,true), StructField(1SDConsumption,DoubleType,true), StructField(timestamp,IntegerType,true), StructField(window,StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)),true))"}], "metadata": {"collapsed": false}}, {"source": "## Create dataframes and queries\n\n1. Create common dataframe on top of rawevents topic", "cell_type": "markdown", "metadata": {}}, {"execution_count": 145, "cell_type": "code", "source": "var commonDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"rawevents\").\noption(\"startingOffsets\",\"latest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), schema).alias(\"consumption\")).\nselect(\"consumption.*\").\nselectExpr(\n    \"house_id as house_id\",\n    \"household_id as household_id\",\n    \"cast(cast(timestamp as int) as timestamp) as timestamp\",\n    \"cast(timestamp as int) as timestamp_int\",\n    \"cast(value as double) as consumption\",\n    \"hour(from_unixtime(cast(eventtimestamp as int))) as LastHour\",\n    \"cast(from_unixtime(cast(eventtimestamp as int)) as date) as Date\"\n).\nwithColumn(\"Id\",concat(\n    col(\"house_id\"),lit(\"_\"),\n    col(\"household_id\"),lit(\"_\"),\n    date_format(col(\"Date\"),\"dd-MM-yyyy\"),lit(\"_\"),\n    col(\"LastHour\"))).\nwithWatermark(\"timestamp\",\"2 minutes\").\ndropDuplicates(\"Id\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "commonDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [house_id: string, household_id: string ... 6 more fields]"}], "metadata": {"collapsed": false}}, {"source": "## Start query to get hourly conumption for household for that hour for that day", "cell_type": "markdown", "metadata": {}}, {"execution_count": 146, "cell_type": "code", "source": "var streamPerHourPerDay = commonDF.\ngroupBy(\n    window($\"timestamp\", \"60 minutes\", \"60 minutes\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"Date\"),col(\"LastHour\"),col(\"Id\")\n).\nagg(\n    sum(\"consumption\").as(\"TotalConsumptionForThatHourAndDay\"),\n    count(\"house_id\").as(\"TotalEvents\")\n).\nwithColumn(\"timestamp\",col(\"window.start\").cast(IntegerType)).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"timestamp\").as[(String,String,Integer)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourPerDay1\").\noption(\"topic\", \"streamPerHourPerDay\").\noutputMode(\"append\").\nstart()\n\n/*format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourPerDay\").\noption(\"topic\", \"streamPerHourPerDay\").\noutputMode(\"append\").\nstart()\n\nformat(\"csv\").\noption(\"path\",\"/example/streamPerHourPerDay\").\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourPerDay\").\noutputMode(\"append\").\nstart() \n*/", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHourPerDay: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5e05fd79"}], "metadata": {"collapsed": false}}, {"execution_count": 120, "cell_type": "code", "source": "println(streamPerHourPerDay.stop)\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "()"}], "metadata": {"collapsed": false}}, {"source": "## Start query to get hourly conumption for a household for that hour from all past events", "cell_type": "markdown", "metadata": {}}, {"execution_count": 147, "cell_type": "code", "source": "var streamPerHour = commonDF.\ngroupBy(\n    window($\"timestamp\", \"60 minutes\", \"60 minutes\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"LastHour\")\n).\nagg(\n    mean(\"consumption\").as(\"MeanConsumptionForThatHour\")\n    ,stddev_pop(\"consumption\").as(\"SDConsumptionForThatHour\")\n).\nwithColumn(\n    \"1SDConsumption\",col(\"MeanConsumptionForThatHour\") + col(\"SDConsumptionForThatHour\")\n).\nwithColumn(\"timestamp\",col(\"window.start\").cast(IntegerType)).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"timestamp\").as[(String,String,Integer)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHour\").\noption(\"topic\", \"streamPerHour\").\noutputMode(\"complete\").\nstart()\n\n/*\nformat(\"csv\").\noption(\"path\",\"/example/streamPerHour\").\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHour\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"complete\").\nstart() \n\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHour\").\noption(\"topic\", \"streamPerHour\").\noutputMode(\"complete\").\nstart()\n*/", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHour: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@77ff0ddb"}], "metadata": {"collapsed": false}}, {"execution_count": 115, "cell_type": "code", "source": "streamPerHour.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res15: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Getting offsets from KafkaSource[Subscribe[rawevents]]\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Start query to get hourly conumption for all households for that hour from all past events", "cell_type": "markdown", "metadata": {}}, {"execution_count": 148, "cell_type": "code", "source": "var streamPerHourAlert2 = commonDF.\ngroupBy(\n    window($\"timestamp\", \"60 minutes\", \"60 minutes\"),\n    col(\"house_id\"),col(\"LastHour\")\n).\nagg(\n    mean(\"consumption\").as(\"MeanConsumptionForThatHour\")\n    ,stddev_pop(\"consumption\").as(\"SDConsumptionForThatHour\")\n).\nwithColumn(\n    \"1SDConsumption\",col(\"MeanConsumptionForThatHour\") + col(\"SDConsumptionForThatHour\")\n).\nwithColumn(\"timestamp\",col(\"window.start\").cast(IntegerType)).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"timestamp\").as[(String,String,Integer)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourAlert2\").\noption(\"topic\", \"streamPerHourAlert2\").\noutputMode(\"complete\").\nstart()\n\n/*\nformat(\"csv\").\noption(\"path\",\"/example/streamPerHourAlert2\").\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourAlert2\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"complete\").\nstart() \n\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourAlert2\").\noption(\"topic\", \"streamPerHourAlert2\").\noutputMode(\"complete\").\nstart()\n*/", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHourAlert2: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@369c5097"}], "metadata": {"collapsed": false}}, {"execution_count": 149, "cell_type": "code", "source": "streamPerHourAlert2.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res40: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Create dataframes and join queries\n1. Consume date from streamPerHourPerDay topic\n2. Consume date from streamPerHour topic\n3. Consume date from streamPerHourAlert2 topic\n4. Prepare a join condition for 1st adn 2nd dataframes for alert type 1 \n5. Prepare a join condition for 1st adn 3rd dataframes for alert type 2", "cell_type": "markdown", "metadata": {}}, {"execution_count": 150, "cell_type": "code", "source": "var kafkastreamPerHourPerDayDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHourPerDay\").\noption(\"startingOffsets\",\"latest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourPerDaySchema).alias(\"streamPerHourPerDay\")).\nselect(\"streamPerHourPerDay.*\").\nselect(\n    col(\"house_id\"),\n    col(\"household_id\"),\n    col(\"LastHour\"),\n    col(\"Id\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"TotalConsumptionForThatHourAndDay\"),\n    col(\"timestamp\").cast(TimestampType)\n).\nwithWatermark(\"Start\",\"5 minutes\")\n\nvar kafkastreamPerHourDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHour\").\noption(\"startingOffsets\",\"latest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourSchema).alias(\"streamPerHour\")).\nselect(\"streamPerHour.*\").\nselect(\n    col(\"house_id\"),\n    col(\"household_id\"),\n    col(\"LastHour\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"1SDConsumption\"),\n    col(\"MeanConsumptionForThatHour\"),\n    col(\"SDConsumptionForThatHour\"),\n    col(\"timestamp\").cast(TimestampType)\n).\nwithWatermark(\"Start\",\"5 minutes\")\n\nvar kafkastreamPerHourAlert2DF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHourAlert2\").\noption(\"startingOffsets\",\"latest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourSchemaAlert2).alias(\"streamPerHour\")).\nselect(\"streamPerHour.*\").\nselect(\n    col(\"house_id\"),\n    col(\"LastHour\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"1SDConsumption\"),\n    col(\"MeanConsumptionForThatHour\"),\n    col(\"SDConsumptionForThatHour\"),\n    col(\"timestamp\").cast(TimestampType)\n).\nwithWatermark(\"Start\",\"5 minutes\")\n\nvar finalResultAlert1 = kafkastreamPerHourPerDayDF.as(\"x\").join(\n    kafkastreamPerHourDF.as(\"y\"),\n    expr(\"\"\"\n        x.house_id = y.house_id AND\n        x.household_id = y.household_id AND\n        x.LastHour = y.LastHour AND\n        x.Start >= y.Start AND x.Start <= y.End AND\n        TotalConsumptionForThatHourAndDay > 1SDConsumption\n        \"\"\"),\n    \"leftOuter\"\n).\nselect(col(\"x.Id\"),col(\"x.house_id\"),col(\"x.household_id\"),col(\"x.LastHour\"),\n       col(\"x.TotalConsumptionForThatHourAndDay\"),\n       col(\"y.1SDConsumption\"),\n       col(\"y.MeanConsumptionForThatHour\"),\n       col(\"y.SDConsumptionForThatHour\")\n      )\n\nvar finalResultAlert2 = kafkastreamPerHourPerDayDF.as(\"x\").join(\n    kafkastreamPerHourAlert2DF.as(\"y\"),\n    expr(\"\"\"\n        x.house_id = y.house_id AND\n        x.LastHour = y.LastHour AND\n        x.Start >= y.Start AND x.Start <= y.End AND\n        TotalConsumptionForThatHourAndDay > 1SDConsumption\n        \"\"\"),\n    \"leftOuter\"\n).\nselect(col(\"x.Id\"),col(\"x.house_id\"),col(\"x.LastHour\"),\n       col(\"x.TotalConsumptionForThatHourAndDay\"),\n       col(\"y.1SDConsumption\"),\n       col(\"y.MeanConsumptionForThatHour\"),\n       col(\"y.SDConsumptionForThatHour\")\n      )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "finalResultAlert2: org.apache.spark.sql.DataFrame = [Id: string, house_id: string ... 5 more fields]"}], "metadata": {"collapsed": false}}, {"source": "## Generate alert type 1", "cell_type": "markdown", "metadata": {}}, {"execution_count": 151, "cell_type": "code", "source": "var alertType1query = finalResultAlert1.\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/AlertType1\").\noption(\"checkpointLocation\", \"/path/chkpointalertType1query\").\noption(\"failOnDataLoss\",\"false\").\ntrigger(Trigger.ProcessingTime(\"5 minutes\")).\noutputMode(\"append\").\nstart() ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "alertType1query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5dd3eb67"}], "metadata": {"collapsed": false}}, {"execution_count": 113, "cell_type": "code", "source": "alertType1query.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res13: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Generate alert type 2", "cell_type": "markdown", "metadata": {}}, {"execution_count": 152, "cell_type": "code", "source": "var alertType2query = finalResultAlert2.\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/AlertType2\").\noption(\"checkpointLocation\", \"/path/chkpointalertType2queryNew\").\noption(\"failOnDataLoss\",\"false\").\ntrigger(Trigger.ProcessingTime(\"5 minutes\")).\noutputMode(\"append\").\nstart() ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "alertType2query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6bbe4869"}], "metadata": {"collapsed": false}}, {"execution_count": 112, "cell_type": "code", "source": "alertType2query.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res12: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"execution_count": 140, "cell_type": "code", "source": "alertType1query.stop\nstreamPerHourPerDay.stop\nstreamPerHour.stop\nstreamPerHourAlert2.stop\nalertType2query.stop", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}