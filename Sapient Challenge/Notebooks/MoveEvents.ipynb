{"nbformat_minor": 2, "cells": [{"execution_count": 207, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1530808215043_0012</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:8088/proxy/application_1530808215043_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:30060/node/containerlogs/container_1530808215043_0012_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1530808215043_0012</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:8088/proxy/application_1530808215043_0012/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:30060/node/containerlogs/container_1530808215043_0012_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 210, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"wn0-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092,wn1-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092\"\nval kafkaTopic=\"rawevents1\"\nval zookeepers = \"zk4-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181,zk5-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "zookeepers: String = zk4-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181,zk5-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181"}], "metadata": {"collapsed": false}}, {"execution_count": 463, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic streamPerHour --zookeeper \"zk4-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181,zk5-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Created topic \"streamPerHour\".\n"}], "metadata": {"collapsed": false}}, {"execution_count": 409, "cell_type": "code", "source": "// Import bits useed for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define a schema for the data\nval schema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"timestamp\", StringType).\nadd(\"value\", StringType).\nadd(\"household_id\", StringType)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "schema: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,StringType,true), StructField(timestamp,StringType,true), StructField(value,StringType,true), StructField(household_id,StringType,true))"}], "metadata": {"collapsed": false}}, {"execution_count": 212, "cell_type": "code", "source": "import org.apache.spark.sql.streaming.Trigger;\n\nval console_op_df = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"startingOffsets\", \"earliest\").\nload()\n                                 \nvar console_op_query = console_op_df.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\").as[(String,String)].\nselect(from_json(col(\"value\"), schema) as \"consumption\").\nwriteStream.\nformat(\"console\").\noutputMode(\"append\").\nstart()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "console_op_query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@755cd645"}], "metadata": {"collapsed": false}}, {"execution_count": 284, "cell_type": "code", "source": "console_op_query.stop()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 457, "cell_type": "code", "source": "val kafka_op_df = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"startingOffsets\",\"latest\").\nload()\n                                 \nvar kafka_op_query = kafka_op_df.selectExpr(\"CAST(key AS STRING) AS key\",\"to_json(struct(*)) AS value\").as[(String,String)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointloc\").\noption(\"topic\", \"cleanseevents\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "kafka_op_query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6cc16343"}], "metadata": {"collapsed": false}}, {"execution_count": 459, "cell_type": "code", "source": "kafka_op_query.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res135: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"execution_count": 327, "cell_type": "code", "source": "val csv_op_df = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"startingOffsets\",\"latest\").\nload()\n\nvar csv_op_query = csv_op_df.selectExpr(\"CAST(key AS STRING) AS Key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), schema).alias(\"consumption\")).\nselect(\"consumption.*\").\nselect(col(\"house_id\").cast(IntegerType)).\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/csvdata\").\noption(\"checkpointLocation\", \"/path/chkpoint1\").\noutputMode(\"append\").\nstart()                                 ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "csv_op_query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@662e98e9"}], "metadata": {"collapsed": false}}, {"execution_count": 447, "cell_type": "code", "source": "csv_op_query.status\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res126: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"execution_count": 456, "cell_type": "code", "source": "csv_op_query.stop()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 448, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /example/csvdata", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 13 items\ndrwxr-xr-x   - livy supergroup          0 2018-07-06 08:41 /example/csvdata/_spark_metadata\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00000-32a66c99-adda-4a03-86c0-27adf4431205-c000.csv\n-rw-r--r--   1 livy supergroup         10 2018-07-06 08:41 /example/csvdata/part-00000-9af34090-e208-44bd-b01f-ab56a549da2c-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00001-56c475ac-012e-469a-be98-cdef914da903-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00002-fc2d0338-85d5-4775-b3a7-f9a128f8a612-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00003-078e2bd7-0dc4-4d8e-8850-db30375c1edf-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00003-722d7334-fb92-4285-907e-0b66cbe6bef6-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00004-b774ddff-c4d7-4cbe-a93d-f1a6eccf1ba2-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00004-d5809ea9-8d9a-46ce-8943-856e2d12f447-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00005-1066d07d-44e7-41b1-b590-a522c3d5ca8c-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00006-075b9182-6bde-4548-aab4-d6c2297aa3da-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00006-4de6a81f-207f-4c2a-8fbc-f4351d0e1a0e-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 08:41 /example/csvdata/part-00007-1fed8e6e-2613-40ad-be0f-df24bd415808-c000.csv\n"}], "metadata": {"collapsed": false}}, {"execution_count": 454, "cell_type": "code", "source": "%%bash\nhdfs dfs -rm -r /path/checkpointloc", "outputs": [{"output_type": "stream", "name": "stderr", "text": "18/07/06 08:45:42 WARN azure.AzureFileSystemThreadPoolExecutor: Disabling threads for Rename operation as thread count 0 is <= 1\n18/07/06 08:45:44 INFO azure.AzureFileSystemThreadPoolExecutor: Time taken for Rename operation is: 1844 ms with threads: 0\n18/07/06 08:45:44 INFO fs.TrashPolicyDefault: Moved: 'wasb://sidsparkcluster@storesidkafkacluster.blob.core.windows.net/path/checkpointloc' to trash at: wasb://sidsparkcluster@storesidkafkacluster.blob.core.windows.net/user/spark/.Trash/Current/path/checkpointloc\n"}], "metadata": {"collapsed": false}}, {"execution_count": 449, "cell_type": "code", "source": "%%bash\nhdfs dfs -cat /example/csvdata/part-00000-9af34090-e208-44bd-b01f-ab56a549da2c-c000.csv", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0\n0\n0\n0\n0\n"}], "metadata": {"collapsed": false}}, {"execution_count": 360, "cell_type": "code", "source": "%%bash\nhdfs dfs -cat /example/jsondata/part-00000-56912788-fe73-401d-8a5b-3c73b3128864-c000.csv", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0,\"{\\\"house_id\\\": \\\"0\\\", \\\"timestamp\\\": \\\"1378272960\\\", \\\"value\\\": \\\"2533.609000000001\\\", \\\"household_id\\\": \\\"0\\\"}\"\n0,\"{\\\"house_id\\\": \\\"0\\\", \\\"timestamp\\\": \\\"1378273020\\\", \\\"value\\\": \\\"2515.039000000001\\\", \\\"household_id\\\": \\\"0\\\"}\"\n0,\"{\\\"house_id\\\": \\\"0\\\", \\\"timestamp\\\": \\\"1378273080\\\", \\\"value\\\": \\\"2454.283000000002\\\", \\\"household_id\\\": \\\"0\\\"}\"\n0,\"{\\\"house_id\\\": \\\"0\\\", \\\"timestamp\\\": \\\"1378273140\\\", \\\"value\\\": \\\"2552.0789999999984\\\", \\\"household_id\\\": \\\"0\\\"}\"\n0,\"{\\\"house_id\\\": \\\"0\\\", \\\"timestamp\\\": \\\"1378273200\\\", \\\"value\\\": \\\"2534.137000000001\\\", \\\"household_id\\\": \\\"0\\\"}\"\n0,\"{\\\"house_id\\\": \\\"0\\\", \\\"timestamp\\\": \\\"1378273260\\\", \\\"value\\\": \\\"2483.291\\\", \\\"household_id\\\": \\\"0\\\"}\"\n"}], "metadata": {"collapsed": false}}, {"execution_count": 228, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --list --zookeeper \"zk4-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181,zk5-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181\"\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "__consumer_offsets\ncleanseevents\nrawevents1\n"}], "metadata": {"collapsed": false}}, {"execution_count": 108, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-consumer-groups.sh  --bootstrap-server \"wn0-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092,wn1-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092\" --list ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "console-consumer-62414\nconsole-consumer-2211\nspark-kafka-source-69018759-e09a-4381-b790-8cc7c3d01e8b--673031480-driver-0\n"}], "metadata": {"collapsed": false}}, {"execution_count": 107, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-consumer-groups.sh  --bootstrap-server \"wn0-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092,wn1-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092\" --describe  --group  \"spark-kafka-source-2020e267-2a5f-4b4a-89f2-f4f009a2a703--1041004857-executor\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Consumer group `spark-kafka-source-2020e267-2a5f-4b4a-89f2-f4f009a2a703--1041004857-executor` does not exist.\n"}], "metadata": {"collapsed": false}}, {"execution_count": 460, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic \"rawevents1\" --zookeeper \"zk4-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181,zk5-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181\"\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Topic rawevents1 is marked for deletion.\nNote: This will have no impact if delete.topic.enable is not set to true.\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}