{"nbformat_minor": 2, "cells": [{"source": "## Start Spark session", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val config = new SparkConf().\nset(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0\").\nset(\"spark.jars.excludes\", \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\")..\nsetAppName(\"EventGeneration\")\n\nval spark = new SparkContext(config)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 2, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1531125931988_0006</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sparkc.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:8088/proxy/application_1531125931988_0006/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sparkc.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:30060/node/containerlogs/container_1531125931988_0006_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nimport org.apache.spark.sql.types._"}], "metadata": {"collapsed": false}}, {"execution_count": 203, "cell_type": "code", "source": "spark.sparkContext.stop", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Drop topics", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic rawevents --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic streamPerHourPerDay --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic streamPerHour --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --delete --topic streamPerHourAlert2 --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Create topics", "cell_type": "markdown", "metadata": {}}, {"execution_count": 251, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic rawevents --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic streamPerHourPerDay --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic streamPerHour --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic streamPerHourAlert2 --zookeeper \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Created topic \"rawevents\".\nCreated topic \"streamPerHourPerDay\".\nCreated topic \"streamPerHour\".\nCreated topic \"streamPerHourAlert2\".\n"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "## Delete all checkpoints directory and output folders", "cell_type": "markdown", "metadata": {}}, {"execution_count": 252, "cell_type": "code", "source": "%%bash\nhdfs dfs -rm -r /path/checkpointstreamPerHour\nhdfs dfs -rm -r /path/checkpointstreamPerHourPerDay\nhdfs dfs -rm -r /path/chkpointalertType1query\nhdfs dfs -rm -r /path/checkpointstreamPerHourAlert2\nhdfs dfs -rm -r /path/chkpointalertType2query\nhdfs dfs -rm -r /example/AlertType1\nhdfs dfs -rm -r /example/AlertType2", "outputs": [{"output_type": "stream", "name": "stderr", "text": "rm: `/path/checkpointstreamPerHour': No such file or directory\nrm: `/path/checkpointstreamPerHourPerDay': No such file or directory\nrm: `/path/chkpointalertType1query': No such file or directory\nrm: `/path/checkpointstreamPerHourAlert2': No such file or directory\nrm: `/path/chkpointalertType2query': No such file or directory\nrm: `/example/AlertType1': No such file or directory\nrm: `/example/AlertType2': No such file or directory\n"}], "metadata": {"collapsed": false}}, {"source": "## Create schemas", "cell_type": "markdown", "metadata": {}}, {"execution_count": 253, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"wn0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:9092,wn1-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:9092,wn2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:9092\"\nval kafkaTopic=\"rawevents\"\nval zookeepers = \"zk0-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk2-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181,zk3-kafka.wwpke2vrtm5e1fnmszmmcb3npe.cx.internal.cloudapp.net:2181\"\n\n// Define a schema for the data\nval schema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"timestamp\", StringType).\nadd(\"value\", StringType).\nadd(\"household_id\", StringType)\n\nvar kafkastreamPerHourPerDaySchema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"household_id\", StringType).\nadd(\"Date\", DateType).\nadd(\"LastHour\", StringType).\nadd(\"Id\", StringType).\nadd(\"TotalConsumptionForThatHourAndDay\",DoubleType).\nadd(\"window\",(new StructType).\n    add(\"start\",TimestampType).\n    add(\"end\",TimestampType)\n   )\n\nvar kafkastreamPerHourSchema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"household_id\", StringType).\nadd(\"LastHour\", StringType).\nadd(\"MeanConsumptionForThatHour\",DoubleType).\nadd(\"SDConsumptionForThatHour\",DoubleType).\nadd(\"1SDConsumption\", DoubleType).\nadd(\"window\",(new StructType).\n    add(\"start\",TimestampType).\n    add(\"end\",TimestampType)\n   )\n\nvar kafkastreamPerHourSchemaAlert2 = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"LastHour\", StringType).\nadd(\"MeanConsumptionForThatHour\",DoubleType).\nadd(\"SDConsumptionForThatHour\",DoubleType).\nadd(\"1SDConsumption\", DoubleType).\nadd(\"window\",(new StructType).\n    add(\"start\",TimestampType).\n    add(\"end\",TimestampType)\n   )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "kafkastreamPerHourSchemaAlert2: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,StringType,true), StructField(LastHour,StringType,true), StructField(MeanConsumptionForThatHour,DoubleType,true), StructField(SDConsumptionForThatHour,DoubleType,true), StructField(1SDConsumption,DoubleType,true), StructField(window,StructType(StructField(start,TimestampType,true), StructField(end,TimestampType,true)),true))"}], "metadata": {"collapsed": false}}, {"source": "## Create dataframes and queries\n\n1. Create common dataframe on top of rawevents topic", "cell_type": "markdown", "metadata": {}}, {"execution_count": 254, "cell_type": "code", "source": "var commonDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"startingOffsets\",\"earliest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), schema).alias(\"consumption\")).\nselect(\"consumption.*\").\nselectExpr(\n    \"house_id\",\n    \"household_id\",\n    \"cast(cast(timestamp as int) as timestamp) as timestamp\",\n    \"cast(value as double) as value\",\n    \"hour(from_unixtime(cast(timestamp as int))) as LastHour\",\n    \"cast(from_unixtime(cast(timestamp as int)) as date) as Date\"\n).\nwithColumn(\"Id\",concat(\n    col(\"house_id\"),lit(\"_\"),\n    col(\"household_id\"),lit(\"_\"),\n    date_format(col(\"Date\"),\"dd-MM-yyyy\"),lit(\"_\"),\n    col(\"LastHour\"))).\nwithWatermark(\"timestamp\",\"1 minute\").\ndropDuplicates(\"Id\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "commonDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [house_id: string, household_id: string ... 5 more fields]"}], "metadata": {"collapsed": false}}, {"source": "## Start query to get hourly conumption for household for that hour for that day", "cell_type": "markdown", "metadata": {}}, {"execution_count": 255, "cell_type": "code", "source": "var streamPerHourPerDay = commonDF.\ngroupBy(\n    window($\"timestamp\", \"1 hour\", \"1 hour\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"Date\"),col(\"LastHour\"),col(\"Id\")\n).\nagg(\n    sum(\"value\").as(\"TotalConsumptionForThatHourAndDay\"),\n    min(\"timestamp\").as(\"MinTimestamp\")\n).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"CAST(MinTimestamp AS string)AS timestamp\").\nas[(String,String,String)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourPerDay\").\noption(\"topic\", \"streamPerHourPerDay\").\noutputMode(\"append\").\nstart()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHourPerDay: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@578dce78"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "var streamPerHourPerDayTesting = commonDF.\ngroupBy(\n    window($\"timestamp\", \"1 hour\", \"1 hour\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"Date\"),col(\"LastHour\"),col(\"Id\")\n).\nagg(\n    sum(\"value\").as(\"TotalConsumptionForThatHourAndDay\"),\n    min(\"timestamp\").as(\"MinTimestamp\")\n).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"CAST(MinTimestamp AS string)AS timestamp\").\nas[(String,String,String)].\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/streamPerHourPerDayTesting\").\noption(\"checkpointLocation\", \"/path/chkpointalertType2query\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart() ", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 268, "cell_type": "code", "source": "streamPerHourPerDay.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res19: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Start query to get hourly conumption for a household for that hour from all past events", "cell_type": "markdown", "metadata": {}}, {"execution_count": 257, "cell_type": "code", "source": "var streamPerHour = commonDF.\ngroupBy(\n    window($\"timestamp\", \"1 hour\", \"1 hour\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"LastHour\")\n).\nagg(\n    mean(\"value\").as(\"MeanConsumptionForThatHour\")\n    ,stddev_pop(\"value\").as(\"SDConsumptionForThatHour\")\n    ,min(\"timestamp\").as(\"MinTimestamp\")\n).\nwithColumn(\n    \"1SDConsumption\"\n    ,col(\"MeanConsumptionForThatHour\") + col(\"SDConsumptionForThatHour\")\n).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"CAST(MinTimestamp AS string)AS timestamp\").\nas[(String,String,String)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHour\").\noption(\"topic\", \"streamPerHour\").\noutputMode(\"complete\").\nstart()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHour: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@6681a1e7"}], "metadata": {"collapsed": false}}, {"execution_count": 166, "cell_type": "code", "source": "streamPerHour.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res18: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Start query to get hourly conumption for all households for that hour from all past events", "cell_type": "markdown", "metadata": {}}, {"execution_count": 258, "cell_type": "code", "source": "var streamPerHourAlert2 = commonDF.\ngroupBy(\n    window($\"timestamp\", \"1 hour\", \"1 hour\"),\n    col(\"house_id\"),col(\"LastHour\")\n).\nagg(\n    mean(\"value\").as(\"MeanConsumptionForThatHour\")\n    ,stddev_pop(\"value\").as(\"SDConsumptionForThatHour\")\n    ,min(\"timestamp\").as(\"MinTimestamp\")\n).\nwithColumn(\n    \"1SDConsumption\"\n    ,col(\"MeanConsumptionForThatHour\") + col(\"SDConsumptionForThatHour\")\n).\nselectExpr(\"cast(house_id as string) as key\",\"to_json(struct(*)) AS value\",\"CAST(MinTimestamp AS string)AS timestamp\").\nas[(String,String,String)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourAlert2\").\noption(\"topic\", \"streamPerHourAlert2\").\noutputMode(\"complete\").\nstart()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHourAlert2: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@5f49a475"}], "metadata": {"collapsed": false}}, {"execution_count": 263, "cell_type": "code", "source": "streamPerHourAlert2.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res14: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Processing new data\",\n  \"isDataAvailable\" : true,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Create dataframes and join queries\n1. Consume date from streamPerHourPerDay topic\n2. Consume date from streamPerHour topic\n3. Consume date from streamPerHourAlert2 topic\n4. Prepare a join condition for 1st adn 2nd dataframes for alert type 1 \n5. Prepare a join condition for 1st adn 3rd dataframes for alert type 2", "cell_type": "markdown", "metadata": {}}, {"execution_count": 259, "cell_type": "code", "source": "var kafkastreamPerHourPerDayDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHourPerDay\").\noption(\"startingOffsets\",\"earliest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourPerDaySchema).alias(\"streamPerHourPerDay\")).\nselect(\"streamPerHourPerDay.*\").\nselect(\n    col(\"house_id\"),\n    col(\"household_id\"),\n    col(\"LastHour\"),\n    col(\"Id\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"TotalConsumptionForThatHourAndDay\")\n).\nwithWatermark(\"Start\",\"5 minute\")\n\nvar kafkastreamPerHourDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHour\").\noption(\"startingOffsets\",\"earliest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourSchema).alias(\"streamPerHour\")).\nselect(\"streamPerHour.*\").\nselect(\n    col(\"house_id\"),\n    col(\"household_id\"),\n    col(\"LastHour\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"1SDConsumption\"),\n    col(\"MeanConsumptionForThatHour\"),\n    col(\"SDConsumptionForThatHour\")\n).\nwithWatermark(\"Start\",\"5 minute\")\n\nvar kafkastreamPerHourAlert2DF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHourAlert2\").\noption(\"startingOffsets\",\"earliest\").\noption(\"failOnDataLoss\",\"false\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourSchemaAlert2).alias(\"streamPerHour\")).\nselect(\"streamPerHour.*\").\nselect(\n    col(\"house_id\"),\n    col(\"LastHour\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"1SDConsumption\"),\n    col(\"MeanConsumptionForThatHour\"),\n    col(\"SDConsumptionForThatHour\")\n).\nwithWatermark(\"Start\",\"5 minute\")\n\nvar finalResultAlert1 = kafkastreamPerHourPerDayDF.as(\"x\").join(\n    kafkastreamPerHourDF.as(\"y\"),\n    expr(\"\"\"\n        x.house_id = y.house_id AND\n        x.household_id = y.household_id AND\n        x.LastHour = y.LastHour AND\n        x.Start = y.Start AND x.End = y.End AND\n        TotalConsumptionForThatHourAndDay > 1SDConsumption\n        \"\"\"),\n    \"leftOuter\"\n).\nselect(col(\"x.Id\"),col(\"x.house_id\"),col(\"x.household_id\"),col(\"x.LastHour\"),\n       col(\"x.TotalConsumptionForThatHourAndDay\"),\n       col(\"y.1SDConsumption\"),\n       col(\"y.MeanConsumptionForThatHour\"),\n       col(\"y.SDConsumptionForThatHour\")\n      )\n\nvar finalResultAlert2 = kafkastreamPerHourPerDayDF.as(\"x\").join(\n    kafkastreamPerHourAlert2DF.as(\"y\"),\n    expr(\"\"\"\n        x.house_id = y.house_id AND\n        x.LastHour = y.LastHour AND\n        x.Start = y.Start AND x.End = y.End AND\n        TotalConsumptionForThatHourAndDay > 1SDConsumption\n        \"\"\"),\n    \"leftOuter\"\n).\nselect(col(\"x.Id\"),col(\"x.house_id\"),col(\"x.LastHour\"),\n       col(\"x.TotalConsumptionForThatHourAndDay\"),\n       col(\"y.1SDConsumption\"),\n       col(\"y.MeanConsumptionForThatHour\"),\n       col(\"y.SDConsumptionForThatHour\")\n      )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "finalResultAlert2: org.apache.spark.sql.DataFrame = [Id: string, house_id: string ... 5 more fields]"}], "metadata": {"collapsed": false}}, {"source": "## Generate alert type 1", "cell_type": "markdown", "metadata": {}}, {"execution_count": 261, "cell_type": "code", "source": "import org.apache.spark.sql.streaming.Trigger\n\nvar alertType1query = finalResultAlert1.\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/AlertType1\").\noption(\"checkpointLocation\", \"/path/chkpointalertType1query\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart() ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "alertType1query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7aa0fdeb"}], "metadata": {"collapsed": false}}, {"execution_count": 206, "cell_type": "code", "source": "alertType1query.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res55: org.apache.spark.sql.streaming.StreamingQueryStatus =\n{\n  \"message\" : \"Getting offsets from KafkaSource[Subscribe[streamPerHourPerDay]]\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : true\n}"}], "metadata": {"collapsed": false}}, {"source": "## Generate alert type 2", "cell_type": "markdown", "metadata": {}}, {"execution_count": 262, "cell_type": "code", "source": "var alertType2query = finalResultAlert2.\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/AlertType2\").\noption(\"checkpointLocation\", \"/path/chkpointalertType2query\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart() ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "alertType2query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@3f5720f"}], "metadata": {"collapsed": false}}, {"execution_count": 231, "cell_type": "code", "source": "alertType2query.status", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res19: Array[org.apache.spark.sql.streaming.StreamingQueryProgress] =\nArray({\n  \"id\" : \"f2e94199-2d8e-4592-8fa4-f6ac5f61b643\",\n  \"runId\" : \"42d73b2d-b997-4bc6-a179-2b38788d74f6\",\n  \"name\" : null,\n  \"timestamp\" : \"2018-07-08T13:39:51.178Z\",\n  \"batchId\" : 0,\n  \"numInputRows\" : 0,\n  \"processedRowsPerSecond\" : 0.0,\n  \"durationMs\" : {\n    \"addBatch\" : 234708,\n    \"getBatch\" : 9,\n    \"getOffset\" : 8279,\n    \"queryPlanning\" : 151,\n    \"triggerExecution\" : 243619,\n    \"walCommit\" : 401\n  },\n  \"eventTime\" : {\n    \"watermark\" : \"1970-01-01T00:00:00.000Z\"\n  },\n  \"stateOperators\" : [ {\n    \"numRowsTotal\" : 0,\n    \"numRowsUpdated\" : 0,\n    \"memoryUsedBytes\" : 57399\n  } ],\n  \"sources\" : [ {\n    \"description\" : \"KafkaSource[Subscribe[streamPerHourPerDay]]\",\n    \"startOffset\" : null,\n    \"endOffset\" : ..."}], "metadata": {"collapsed": false}}, {"execution_count": 241, "cell_type": "code", "source": "alertType1query.stop\nstreamPerHourPerDay.stop\nstreamPerHour.stop\nstreamPerHourAlert2.stop\nalertType2query.stop", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 74, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /example/AlertType2", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 107 items\ndrwxr-xr-x   - livy supergroup          0 2018-07-07 20:31 /example/AlertType2/_spark_metadata\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:08 /example/AlertType2/part-00000-13944819-93f1-4091-86f5-41c8a4b87c7b-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 19:59 /example/AlertType2/part-00000-15aff356-25d4-40b2-a741-07bbd2b256bb-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 19:50 /example/AlertType2/part-00000-1c525fbd-3a7b-4507-8559-48cd1da5e05d-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:05 /example/AlertType2/part-00000-337f1dbe-020e-49ce-83df-78af162cf0be-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:18 /example/AlertType2/part-00000-3ba1d0d9-e5f4-421c-bacf-5dcfd4b9e3bd-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:28 /example/AlertType2/part-00000-5b307dee-ca54-4e5f-8ca4-775e9b0c3e9a-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:30 /example/AlertType2/part-00000-5ec27552-97fb-4546-9c68-c9a855fcbb42-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:21 /example/AlertType2/part-00000-6eb7555f-4b5a-4a1c-aa44-05975d693cfd-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:25 /example/AlertType2/part-00000-84b6929b-7c30-43c7-9bf5-81f29616027a-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 19:44 /example/AlertType2/part-00000-a5386d89-ba67-4bc9-baa9-11083ee29bda-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:12 /example/AlertType2/part-00000-b16570c5-68e2-44f2-8af1-e4a039919df5-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 19:53 /example/AlertType2/part-00000-bb8c2e18-e750-4b88-90b6-7466a15cf265-c000.csv\n-rw-r--r--   1 livy supergroup         85 2018-07-07 20:02 /example/AlertType2/part-00000-c9b13960-8dbc-4176-8ed7-278c89b14ccc-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 19:56 /example/AlertType2/part-00000-d2296dc4-4720-48b9-b88d-94501f1f7680-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-07 20:15 /example/AlertType2/part-00000-f3a4e03a-2995-4acd-aa9d-519e7d393d1c-c000.csv\n-rw-r--r--   1 livy supergroup       5288 2018-07-07 19:59 /example/AlertType2/part-00014-3c35e812-46d1-481c-a7d2-c0f866b37c4e-c000.csv\n-rw-r--r--   1 livy supergroup       4519 2018-07-07 20:02 /example/AlertType2/part-00014-71ef4e7a-85b3-4b34-b49b-51ba8943779a-c000.csv\n-rw-r--r--   1 livy supergroup       4966 2018-07-07 20:02 /example/AlertType2/part-00019-8395a3e8-75c3-4df4-b5b2-287ee292bf56-c000.csv\n-rw-r--r--   1 livy supergroup       4978 2018-07-07 19:59 /example/AlertType2/part-00019-da9dca6b-44bc-41d0-b1b1-4c08934ac8ae-c000.csv\n-rw-r--r--   1 livy supergroup        820 2018-07-07 20:02 /example/AlertType2/part-00020-01ea255d-40c7-4de1-8a24-3ef9b4219548-c000.csv\n-rw-r--r--   1 livy supergroup         81 2018-07-07 20:02 /example/AlertType2/part-00025-eaa91a10-eff8-4d40-9dce-c4a336b6efb5-c000.csv\n-rw-r--r--   1 livy supergroup       4786 2018-07-07 19:59 /example/AlertType2/part-00026-2bf8c4e3-9163-48b8-b0fe-4ffac59a8ca7-c000.csv\n-rw-r--r--   1 livy supergroup       4664 2018-07-07 20:02 /example/AlertType2/part-00026-50125cf2-1d6d-4b94-a621-28947e1318db-c000.csv\n-rw-r--r--   1 livy supergroup       5212 2018-07-07 19:59 /example/AlertType2/part-00028-424ea328-c0f0-4c28-b008-59e4a9558142-c000.csv\n-rw-r--r--   1 livy supergroup       4693 2018-07-07 20:02 /example/AlertType2/part-00028-c3e59899-8601-4af1-a3dd-a6e4d1202589-c000.csv\n-rw-r--r--   1 livy supergroup       4784 2018-07-07 19:59 /example/AlertType2/part-00029-bc271b47-efec-4794-8a6d-7066e6a1f7d8-c000.csv\n-rw-r--r--   1 livy supergroup       4205 2018-07-07 20:02 /example/AlertType2/part-00029-ecc22f54-5af7-4673-9714-52da07c26a28-c000.csv\n-rw-r--r--   1 livy supergroup         80 2018-07-07 20:02 /example/AlertType2/part-00031-53df059c-570b-493e-aaef-5dc6f590ed96-c000.csv\n-rw-r--r--   1 livy supergroup         70 2018-07-07 20:02 /example/AlertType2/part-00035-2497740e-f6fd-49db-bacb-f709502d3878-c000.csv\n-rw-r--r--   1 livy supergroup       4263 2018-07-07 20:02 /example/AlertType2/part-00040-1e8873c0-fde2-492b-8260-624079f48539-c000.csv\n-rw-r--r--   1 livy supergroup       5418 2018-07-07 19:59 /example/AlertType2/part-00040-ead99ab2-f701-46f7-abcd-89712873bdf9-c000.csv\n-rw-r--r--   1 livy supergroup         74 2018-07-07 20:02 /example/AlertType2/part-00041-724d6c16-94d6-4343-b79f-6472b99f38da-c000.csv\n-rw-r--r--   1 livy supergroup        779 2018-07-07 20:02 /example/AlertType2/part-00052-bf747c4b-8154-48f9-982c-d2478d2b6761-c000.csv\n-rw-r--r--   1 livy supergroup        798 2018-07-07 20:02 /example/AlertType2/part-00057-5f6e04c0-5d8b-4a1d-931d-742d429e64bb-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:05 /example/AlertType2/part-00058-0245efc6-f712-45aa-bb91-92384b5ef118-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:29 /example/AlertType2/part-00058-0e0d287d-9479-4a5f-8637-5ca9fede6e35-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:25 /example/AlertType2/part-00058-17954750-465d-4238-ab83-b430fcd87652-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:30 /example/AlertType2/part-00058-1ea9068f-3ed2-4445-aee5-5783b2971259-c000.csv\n-rw-r--r--   1 livy supergroup       4464 2018-07-07 20:02 /example/AlertType2/part-00058-3c0704c6-3abf-4131-9d97-743c110bd86d-c000.csv\n-rw-r--r--   1 livy supergroup       9403 2018-07-07 19:59 /example/AlertType2/part-00058-6598afb5-851e-4e95-8f0b-0a65c087db69-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:22 /example/AlertType2/part-00058-8f5fcc2d-a83a-4db4-b8e8-4e15ae20ddf4-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:09 /example/AlertType2/part-00058-983e7e96-ef26-409a-9edc-a3d79674b4a3-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:18 /example/AlertType2/part-00058-bd7d9d62-e74b-485e-9bf2-d596a54b281e-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:12 /example/AlertType2/part-00058-cc709a73-40e1-40a3-b48c-994525796ffd-c000.csv\n-rw-r--r--   1 livy supergroup         95 2018-07-07 20:15 /example/AlertType2/part-00058-e8ad278c-558a-488f-83da-f8b457b73a64-c000.csv\n-rw-r--r--   1 livy supergroup         80 2018-07-07 20:02 /example/AlertType2/part-00063-bbf0e0ae-6a9d-4d09-8c71-a9090df3938f-c000.csv\n-rw-r--r--   1 livy supergroup         86 2018-07-07 20:02 /example/AlertType2/part-00065-9ca62b1a-a920-45f0-a876-502420f2ef9e-c000.csv\n-rw-r--r--   1 livy supergroup         85 2018-07-07 20:02 /example/AlertType2/part-00068-c1a7c90b-336c-483f-a182-caaab7d2c20b-c000.csv\n-rw-r--r--   1 livy supergroup        791 2018-07-07 20:02 /example/AlertType2/part-00072-753af0fa-f533-4899-87ca-9f6883792e8b-c000.csv\n-rw-r--r--   1 livy supergroup         73 2018-07-07 20:02 /example/AlertType2/part-00074-8f6bfab4-25e1-4414-a657-702f5585a7bd-c000.csv\n-rw-r--r--   1 livy supergroup        816 2018-07-07 20:02 /example/AlertType2/part-00075-d4074c41-582f-40da-840d-3277ff8988ce-c000.csv\n-rw-r--r--   1 livy supergroup         76 2018-07-07 20:02 /example/AlertType2/part-00078-cfb69f13-2ff7-48e9-a2a1-84db28d5a4cf-c000.csv\n-rw-r--r--   1 livy supergroup         71 2018-07-07 20:02 /example/AlertType2/part-00081-7e46ee78-a3bc-43f7-b3dc-47f37cbf1e20-c000.csv\n-rw-r--r--   1 livy supergroup         86 2018-07-07 20:02 /example/AlertType2/part-00082-ba2674f4-8027-4507-b48c-9fbf33629b23-c000.csv\n-rw-r--r--   1 livy supergroup       5022 2018-07-07 20:02 /example/AlertType2/part-00084-1888594e-d591-47b4-a3a6-70834450470e-c000.csv\n-rw-r--r--   1 livy supergroup       4806 2018-07-07 19:59 /example/AlertType2/part-00084-97e59922-6bb6-42ae-a849-4f3aa2bf87e6-c000.csv\n-rw-r--r--   1 livy supergroup       4732 2018-07-07 20:02 /example/AlertType2/part-00085-1d5d7afd-7cc3-45bc-a14f-3c9cb928083e-c000.csv\n-rw-r--r--   1 livy supergroup       6030 2018-07-07 19:59 /example/AlertType2/part-00085-c4aa8b7f-cfaa-45b7-baee-d286d7944739-c000.csv\n-rw-r--r--   1 livy supergroup        779 2018-07-07 20:02 /example/AlertType2/part-00092-b1e76557-a339-4c2a-a9f2-adf0fc08ed5e-c000.csv\n-rw-r--r--   1 livy supergroup         70 2018-07-07 20:02 /example/AlertType2/part-00094-99330e96-6129-486b-a38f-caf5c94b3ed1-c000.csv\n-rw-r--r--   1 livy supergroup        796 2018-07-07 20:02 /example/AlertType2/part-00097-fa641850-098e-4b59-b1b2-f2e8f9b3bb3e-c000.csv\n-rw-r--r--   1 livy supergroup       4586 2018-07-07 20:03 /example/AlertType2/part-00105-7e6e2269-c87f-4d59-a218-b9384a47a4f0-c000.csv\n-rw-r--r--   1 livy supergroup       5224 2018-07-07 19:59 /example/AlertType2/part-00105-f5a8c293-544e-4a15-ac84-7fa074e2b5c1-c000.csv\n-rw-r--r--   1 livy supergroup       4461 2018-07-07 20:02 /example/AlertType2/part-00107-37858e7c-556a-4fae-a7e2-a457a3c4bfd7-c000.csv\n-rw-r--r--   1 livy supergroup       6224 2018-07-07 19:59 /example/AlertType2/part-00107-46da963c-9d72-4403-ac5e-d0e3039f61e6-c000.csv\n-rw-r--r--   1 livy supergroup       5968 2018-07-07 19:59 /example/AlertType2/part-00108-2d877cf1-04b7-4be3-8ff7-c8ba9302e0f0-c000.csv\n-rw-r--r--   1 livy supergroup       4660 2018-07-07 20:02 /example/AlertType2/part-00108-30605401-3d39-463c-a68f-d62df48da1e1-c000.csv\n-rw-r--r--   1 livy supergroup        786 2018-07-07 20:03 /example/AlertType2/part-00109-a93f2715-c1ab-4f40-bc7d-744835f03582-c000.csv\n-rw-r--r--   1 livy supergroup        842 2018-07-07 20:02 /example/AlertType2/part-00111-84fa3ff8-da15-4123-aa69-aaf0609c47e6-c000.csv\n-rw-r--r--   1 livy supergroup        852 2018-07-07 20:03 /example/AlertType2/part-00114-d6ec15be-f281-4a06-b302-0dd4724da6c9-c000.csv\n-rw-r--r--   1 livy supergroup       5353 2018-07-07 19:59 /example/AlertType2/part-00116-ca8728cd-4289-49f4-83fe-54a7b1f02019-c000.csv\n-rw-r--r--   1 livy supergroup       3901 2018-07-07 20:03 /example/AlertType2/part-00116-e714307a-9619-4636-8c1b-7f09ca5e7181-c000.csv\n-rw-r--r--   1 livy supergroup       1579 2018-07-07 20:03 /example/AlertType2/part-00121-e57b5a04-671c-403b-b167-deec8a40e400-c000.csv\n-rw-r--r--   1 livy supergroup         73 2018-07-07 20:03 /example/AlertType2/part-00128-22c749ce-04cc-4e3e-9847-0bd7cb5b367a-c000.csv\n-rw-r--r--   1 livy supergroup        827 2018-07-07 20:03 /example/AlertType2/part-00135-91662935-a4c7-460e-b9d7-c065e9c69715-c000.csv\n-rw-r--r--   1 livy supergroup        833 2018-07-07 20:03 /example/AlertType2/part-00138-97448aa2-f367-4325-955a-e97b721f4fec-c000.csv\n-rw-r--r--   1 livy supergroup       4896 2018-07-07 20:03 /example/AlertType2/part-00140-d69ea81e-7134-46d3-a618-72aba8d45269-c000.csv\n-rw-r--r--   1 livy supergroup       5842 2018-07-07 20:00 /example/AlertType2/part-00140-f707293f-d356-4081-86d7-29cb0eca31ae-c000.csv\n-rw-r--r--   1 livy supergroup       5714 2018-07-07 20:00 /example/AlertType2/part-00142-376e4db6-99d3-4c06-b43c-91220d25262c-c000.csv\n-rw-r--r--   1 livy supergroup       3979 2018-07-07 20:03 /example/AlertType2/part-00142-de090ac7-465e-464b-8c14-b14e06beafe7-c000.csv\n-rw-r--r--   1 livy supergroup       4360 2018-07-07 20:03 /example/AlertType2/part-00145-1c44f4dc-d20a-42bd-99b9-7e07a8b6e161-c000.csv\n-rw-r--r--   1 livy supergroup       4478 2018-07-07 20:00 /example/AlertType2/part-00145-8c7d608f-8a6e-4625-9204-49e20b079e17-c000.csv\n-rw-r--r--   1 livy supergroup         65 2018-07-07 20:03 /example/AlertType2/part-00146-92ceaf8d-9c34-48e1-a063-1f6d4cd890bc-c000.csv\n-rw-r--r--   1 livy supergroup         70 2018-07-07 20:03 /example/AlertType2/part-00148-974c08ee-291b-4cae-a6a8-e89455e27665-c000.csv\n-rw-r--r--   1 livy supergroup        845 2018-07-07 20:03 /example/AlertType2/part-00152-9255d84d-94b0-490f-9e92-c7f9661952ab-c000.csv\n-rw-r--r--   1 livy supergroup        759 2018-07-07 20:03 /example/AlertType2/part-00159-a7d52d26-d587-4dc1-bd2e-e50137ae8263-c000.csv\n-rw-r--r--   1 livy supergroup       4278 2018-07-07 20:00 /example/AlertType2/part-00160-22485b9d-fced-41aa-81a3-9d7f8cffba48-c000.csv\n-rw-r--r--   1 livy supergroup       4320 2018-07-07 20:03 /example/AlertType2/part-00160-f3473e60-2dc4-4ae7-9722-9665eeebcc18-c000.csv\n-rw-r--r--   1 livy supergroup       5138 2018-07-07 20:03 /example/AlertType2/part-00162-13760571-58fe-4301-b310-acbe3a98df2c-c000.csv\n-rw-r--r--   1 livy supergroup       4754 2018-07-07 20:00 /example/AlertType2/part-00162-33eec74e-c9fd-446b-8e8e-b427dd893a19-c000.csv\n-rw-r--r--   1 livy supergroup        801 2018-07-07 20:03 /example/AlertType2/part-00163-5a206ee2-ab76-4840-a51b-2fa961be29ec-c000.csv\n-rw-r--r--   1 livy supergroup       4386 2018-07-07 20:03 /example/AlertType2/part-00164-36451887-519e-4aae-89b4-970ae37dc31a-c000.csv\n-rw-r--r--   1 livy supergroup       5418 2018-07-07 20:00 /example/AlertType2/part-00164-d8b4d09f-fa98-4f55-9d69-28795f395880-c000.csv\n-rw-r--r--   1 livy supergroup         81 2018-07-07 20:03 /example/AlertType2/part-00166-1d20d32d-5eb3-48dc-97f2-857ac3e8fd14-c000.csv\n-rw-r--r--   1 livy supergroup       1497 2018-07-07 20:03 /example/AlertType2/part-00167-116011fc-78e2-436c-9425-8c0e114962e7-c000.csv\n-rw-r--r--   1 livy supergroup         80 2018-07-07 20:03 /example/AlertType2/part-00172-5085308a-f02a-4301-91a2-628bfe08c2e9-c000.csv\n-rw-r--r--   1 livy supergroup         85 2018-07-07 20:03 /example/AlertType2/part-00178-75effc6d-0930-4935-8935-70ced3d70b6a-c000.csv\n-rw-r--r--   1 livy supergroup       4646 2018-07-07 20:00 /example/AlertType2/part-00182-47a446a2-9fd5-44e9-9888-c2952ecdc443-c000.csv\n-rw-r--r--   1 livy supergroup       5116 2018-07-07 20:03 /example/AlertType2/part-00182-77c51a65-359b-4b21-ae6c-a94c6abc656f-c000.csv\n-rw-r--r--   1 livy supergroup       6136 2018-07-07 20:00 /example/AlertType2/part-00183-4f0e1380-c72d-4aac-b718-e43728e8ff1d-c000.csv\n-rw-r--r--   1 livy supergroup       4183 2018-07-07 20:03 /example/AlertType2/part-00183-b0507979-343b-4ec7-b909-a1e0ce5b5cef-c000.csv\n-rw-r--r--   1 livy supergroup        761 2018-07-07 20:03 /example/AlertType2/part-00184-48955730-b56c-406c-a233-d988831d7b56-c000.csv\n-rw-r--r--   1 livy supergroup        880 2018-07-07 20:03 /example/AlertType2/part-00192-9adbdb63-77b7-4e30-b1c7-c72b9201f2af-c000.csv\n-rw-r--r--   1 livy supergroup       8818 2018-07-07 20:03 /example/AlertType2/part-00194-2513c456-f162-43b2-a01f-c07cce4e7bcf-c000.csv\n-rw-r--r--   1 livy supergroup      10778 2018-07-07 20:00 /example/AlertType2/part-00194-e2223f4f-3398-49ac-adb2-8b2ee2235333-c000.csv\n-rw-r--r--   1 livy supergroup         81 2018-07-07 20:03 /example/AlertType2/part-00199-e1817c35-692d-43c2-b5bf-c3676cf3c8cc-c000.csv\n"}], "metadata": {"collapsed": false}}, {"execution_count": 67, "cell_type": "code", "source": "%%bash\nhdfs dfs -cat /example/AlertType2/part-00000-c9b13960-8dbc-4176-8ed7-278c89b14ccc-c000.csv", "outputs": [{"output_type": "stream", "name": "stdout", "text": "2_0_1-9-2013_22,2,22,668.7830000000001,,,\n2_0_2-9-2013_22,2,22,3840.0310000000013,,,\n"}], "metadata": {"collapsed": false}}, {"execution_count": 99, "cell_type": "code", "source": "%%bash\nhdfs dfs -getmerge /example/AlertType1 /home/sshuser/AlertType1.csv", "outputs": [{"output_type": "stream", "name": "stderr", "text": "getmerge: /home/sshuser/AlertType1.csv (Permission denied)\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}