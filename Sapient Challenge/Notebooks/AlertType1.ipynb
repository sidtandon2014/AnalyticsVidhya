{"nbformat_minor": 2, "cells": [{"execution_count": 88, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1530808215043_0015</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:8088/proxy/application_1530808215043_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:30060/node/containerlogs/container_1530808215043_0015_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>7</td><td>application_1530808215043_0015</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:8088/proxy/application_1530808215043_0015/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn0-sidspa.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:30060/node/containerlogs/container_1530808215043_0015_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"execution_count": 115, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"wn0-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092,wn1-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092\"\nval kafkaTopic=\"rawevents\"\nval zookeepers = \"zk4-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181,zk5-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:2181\"\n\n// Import bits useed for declaring schemas and working with JSON data\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Define a schema for the data\nval schema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"timestamp\", StringType).\nadd(\"value\", StringType).\nadd(\"household_id\", StringType)\n\nvar kafkastreamPerHourPerDaySchema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"household_id\", StringType).\nadd(\"Date\", DateType).\nadd(\"LastHour\", IntegerType).\nadd(\"Id\", StringType).\nadd(\"TotalConsumptionForThatHourAndDay\",DoubleType).\nadd(\"window\",(new StructType).\n    add(\"start\",DateType).\n    add(\"end\",DateType)\n   )\n\nvar kafkastreamPerHourSchema = (new StructType).\nadd(\"house_id\", StringType).\nadd(\"household_id\", StringType).\nadd(\"LastHour\", StringType).\nadd(\"Id\", StringType).\nadd(\"MeanConsumptionForThatHour\",DoubleType).\nadd(\"SDConsumptionForThatHour\",DoubleType).\nadd(\"1SDConsumption\", DoubleType).\nadd(\"window\",(new StructType).\n    add(\"start\",DateType).\n    add(\"end\",DateType)\n   )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "kafkastreamPerHourSchema: org.apache.spark.sql.types.StructType = StructType(StructField(house_id,StringType,true), StructField(household_id,StringType,true), StructField(LastHour,StringType,true), StructField(Id,StringType,true), StructField(MeanConsumptionForThatHour,DoubleType,true), StructField(SDConsumptionForThatHour,DoubleType,true), StructField(1SDConsumption,DoubleType,true), StructField(window,StructType(StructField(start,DateType,true), StructField(end,DateType,true)),true))"}], "metadata": {"collapsed": false}}, {"execution_count": 97, "cell_type": "code", "source": "var commonDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"startingOffsets\",\"earliest\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), schema).alias(\"consumption\")).\nselect(\"consumption.*\").\nselectExpr(\n    \"house_id\",\n    \"household_id\",\n    \"cast(cast(timestamp as int) as timestamp) as timestamp\",\n    \"cast(value as double) as value\",\n    \"hour(from_unixtime(timestamp)) as Lasthour\",\n    \"from_unixtime(timestamp) as FullDateTime\",\n    \"cast(from_unixtime(timestamp) as date) as Date\"\n).\nwithColumn(\"Id\",concat(col(\"house_id\"),lit(\"_\"),col(\"household_id\"),lit(\"_\"),col(\"Date\"),lit(\"_\"),col(\"Lasthour\"))).\nwithWatermark(\"timestamp\",\"10 minute\").\ndropDuplicates(\"Id\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "commonDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [house_id: string, household_id: string ... 6 more fields]"}], "metadata": {"collapsed": false}}, {"execution_count": 111, "cell_type": "code", "source": "var streamPerHourPerDay = commonDF.\ngroupBy(\n    window($\"timestamp\", \"1 hour\", \"1 hour\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"Date\"),col(\"Lasthour\"),col(\"Id\")\n).\nagg(\n    sum(\"value\").as(\"TotalConsumptionForThatHourAndDay\")\n).\nselectExpr(\"cast(Id as string) as key\",\"to_json(struct(*)) AS value\").as[(String,String)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHourPerDay\").\noption(\"topic\", \"streamPerHourPerDay\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHourPerDay: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@227ea717"}], "metadata": {"collapsed": false}}, {"execution_count": 114, "cell_type": "code", "source": "var streamPerHour = commonDF.\ngroupBy(\n    window($\"timestamp\", \"1 hour\", \"1 hour\"),\n    col(\"house_id\"),col(\"household_id\"),col(\"Lasthour\")\n).\nagg(\n    mean(\"value\").as(\"MeanConsumptionForThatHour\")\n    ,stddev_pop(\"value\").as(\"SDConsumptionForThatHour\")\n).\nwithColumn(\n    \"1SDConsumption\"\n    ,col(\"MeanConsumptionForThatHour\") + col(\"SDConsumptionForThatHour\")\n).\nwithColumn(\"Id\",concat(col(\"house_id\"),lit(\"_\"),col(\"household_id\"),lit(\"_\"),col(\"Lasthour\"))).\nselectExpr(\"cast(Id as string) as key\",\"to_json(struct(*)) AS value\").as[(String,String)].\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointstreamPerHour\").\noption(\"topic\", \"streamPerHour\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"update\").\nstart()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "streamPerHour: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@21061a9d"}], "metadata": {"collapsed": false}}, {"execution_count": 124, "cell_type": "code", "source": "var kafkastreamPerHourPerDayDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHourPerDay\").\noption(\"startingOffsets\",\"latest\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourPerDaySchema).alias(\"streamPerHourPerDay\")).\nselect(\"streamPerHourPerDay.*\").\nselect(\n    col(\"house_id\"),\n    col(\"household_id\"),\n    col(\"Lasthour\"),\n    col(\"Id\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"TotalConsumptionForThatHourAndDay\")\n)\n\nvar kafkastreamPerHourDF = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"streamPerHour\").\noption(\"startingOffsets\",\"latest\").\nload().\nselectExpr(\"CAST(key AS STRING) AS key\",\"CAST(value AS STRING) AS value\").as[(String,String)].\nselect(from_json(col(\"value\"), kafkastreamPerHourSchema).alias(\"streamPerHour\")).\nselect(\"streamPerHour.*\").\nselect(\n    col(\"house_id\"),\n    col(\"household_id\"),\n    col(\"Lasthour\"),\n    col(\"window.start\").as(\"Start\"),\n    col(\"window.end\").as(\"End\"),\n    col(\"1SDConsumption\"),\n    col(\"MeanConsumptionForThatHour\"),\n    col(\"SDConsumptionForThatHour\")\n)\n\nvar finalResult = kafkastreamPerHourPerDayDF.as(\"x\").join(\n    kafkastreamPerHourDF.as(\"y\"),\n    expr(\"\"\"\n        x.house_id = y.house_id AND\n        x.household_id = y.household_id AND\n        x.Lasthour = y.Lasthour AND\n        x.Start >= y.Start AND x.Start < y.End AND\n        TotalConsumptionForThatHourAndDay > 1SDConsumption\n        \"\"\")\n).\nselect(col(\"x.Id\"))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "finalResult: org.apache.spark.sql.DataFrame = [Id: string]"}], "metadata": {"collapsed": false}}, {"execution_count": 125, "cell_type": "code", "source": "var query = finalResult.\nwriteStream.\nformat(\"csv\").\noption(\"path\",\"/example/AlertType1\").\noption(\"checkpointLocation\", \"/path/chkpointFinalResult\").\noutputMode(\"append\").\nstart() ", "outputs": [{"output_type": "stream", "name": "stderr", "text": "org.apache.spark.sql.AnalysisException: Inner join between two streaming DataFrames/Datasets is not supported;;\nJoin Inner, ((((house_id#1625 = house_id#1675) && (household_id#1626 = household_id#1676)) && (Lasthour#1628 = cast(Lasthour#1677 as int))) && (((Start#1640 >= Start#1692) && (Start#1640 < End#1693)) && (TotalConsumptionForThatHourAndDay#1630 > 1SDConsumption#1681)))\n:- SubqueryAlias x\n:  +- Project [house_id#1625, household_id#1626, Lasthour#1628, Id#1629, window#1631.start AS Start#1640, TotalConsumptionForThatHourAndDay#1630]\n:     +- Project [streamPerHourPerDay#1622.house_id AS house_id#1625, streamPerHourPerDay#1622.household_id AS household_id#1626, streamPerHourPerDay#1622.Date AS Date#1627, streamPerHourPerDay#1622.LastHour AS LastHour#1628, streamPerHourPerDay#1622.Id AS Id#1629, streamPerHourPerDay#1622.TotalConsumptionForThatHourAndDay AS TotalConsumptionForThatHourAndDay#1630, streamPerHourPerDay#1622.window AS window#1631]\n:        +- Project [jsontostructs(StructField(house_id,StringType,true), StructField(household_id,StringType,true), StructField(Date,DateType,true), StructField(LastHour,IntegerType,true), StructField(Id,StringType,true), StructField(TotalConsumptionForThatHourAndDay,DoubleType,true), StructField(window,StructType(StructField(start,DateType,true), StructField(end,DateType,true)),true), value#1615, Some(Etc/UTC)) AS streamPerHourPerDay#1622]\n:           +- Project [cast(key#1599 as string) AS key#1614, cast(value#1600 as string) AS value#1615]\n:              +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@55462f7b,kafka,List(),None,List(),None,Map(startingOffsets -> latest, subscribe -> streamPerHourPerDay, kafka.bootstrap.servers -> wn0-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092,wn1-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092),None), kafka, [key#1599, value#1600, topic#1601, partition#1602, offset#1603L, timestamp#1604, timestampType#1605]\n+- SubqueryAlias y\n   +- Project [house_id#1675, household_id#1676, Lasthour#1677, window#1682.start AS Start#1692, window#1682.end AS End#1693, 1SDConsumption#1681, MeanConsumptionForThatHour#1679, SDConsumptionForThatHour#1680]\n      +- Project [streamPerHour#1672.house_id AS house_id#1675, streamPerHour#1672.household_id AS household_id#1676, streamPerHour#1672.LastHour AS LastHour#1677, streamPerHour#1672.Id AS Id#1678, streamPerHour#1672.MeanConsumptionForThatHour AS MeanConsumptionForThatHour#1679, streamPerHour#1672.SDConsumptionForThatHour AS SDConsumptionForThatHour#1680, streamPerHour#1672.1SDConsumption AS 1SDConsumption#1681, streamPerHour#1672.window AS window#1682]\n         +- Project [jsontostructs(StructField(house_id,StringType,true), StructField(household_id,StringType,true), StructField(LastHour,StringType,true), StructField(Id,StringType,true), StructField(MeanConsumptionForThatHour,DoubleType,true), StructField(SDConsumptionForThatHour,DoubleType,true), StructField(1SDConsumption,DoubleType,true), StructField(window,StructType(StructField(start,DateType,true), StructField(end,DateType,true)),true), value#1665, Some(Etc/UTC)) AS streamPerHour#1672]\n            +- Project [cast(key#1649 as string) AS key#1664, cast(value#1650 as string) AS value#1665]\n               +- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@55462f7b,kafka,List(),None,List(),None,Map(startingOffsets -> latest, subscribe -> streamPerHour, kafka.bootstrap.servers -> wn0-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092,wn1-kafka.y4m1vjdw55he5ghlbslra2ouva.cx.internal.cloudapp.net:9092),None), kafka, [key#1649, value#1650, topic#1651, partition#1652, offset#1653L, timestamp#1654, timestampType#1655]\n\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:297)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:223)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForStreaming$2.apply(UnsupportedOperationChecker.scala:131)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForStreaming(UnsupportedOperationChecker.scala:131)\n  at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:232)\n  at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:278)\n  at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:276)\n  ... 62 elided\n"}], "metadata": {"collapsed": false}}, {"execution_count": 91, "cell_type": "code", "source": "query.status", "outputs": [{"output_type": "stream", "name": "stderr", "text": "<console>:33: error: not found: value streamPerHourPerDay\n       streamPerHourPerDay.stop\n       ^\n"}], "metadata": {"collapsed": false}}, {"execution_count": 83, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /example/csvdata", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 10 items\ndrwxr-xr-x   - livy supergroup          0 2018-07-06 15:39 /example/csvdata/_spark_metadata\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00000-978a5c09-8efa-4d89-9083-93122c8f68bd-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00001-b5fa8eda-94d9-4eae-a34b-6ea24c8bfadd-c000.csv\n-rw-r--r--   1 livy supergroup        100 2018-07-06 15:39 /example/csvdata/part-00002-6c325cad-b52a-48f6-9d61-409cfd4ab6c8-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00003-59e418e9-62c6-44ef-aa4a-7f40dadb772e-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00004-1a275171-c400-4e30-8675-cbdac9e7c4bd-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00005-fde47d31-5882-4008-8751-5ba55bc493b3-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00006-0a9c3d59-d71d-4bfd-94f2-bec21bb1e89a-c000.csv\n-rw-r--r--   1 livy supergroup          0 2018-07-06 15:39 /example/csvdata/part-00007-1dcbf4d7-d4c7-4cc7-bc17-15eb2fcd7bfc-c000.csv\n-rw-r--r--   1 livy supergroup         99 2018-07-06 15:39 /example/csvdata/part-00181-e20e8be1-cfab-4ffa-bb30-ca18591b9945-c000.csv\n"}], "metadata": {"collapsed": false}}, {"execution_count": 85, "cell_type": "code", "source": "%%bash\nhdfs dfs -cat /example/csvdata/part-00002-6c325cad-b52a-48f6-9d61-409cfd4ab6c8-c000.csv", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0,0,2013-09-02T13:00:00.000Z,10045.438999999997,13,2013-09-02 13:00:00,2013-09-02,0_0_2013-09-02_13\n"}], "metadata": {"collapsed": false}}, {"execution_count": 109, "cell_type": "code", "source": "%%bash\nhdfs dfs -rm -r /path/chkpoint\nhdfs dfs -rm -r /example/AlertType1", "outputs": [{"output_type": "stream", "name": "stderr", "text": "rm: `/path/chkpoint': No such file or directory\nrm: `/example/AlertType1': No such file or directory\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}