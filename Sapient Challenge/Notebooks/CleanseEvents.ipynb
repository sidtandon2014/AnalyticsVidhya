{"nbformat_minor": 2, "cells": [{"execution_count": 272, "cell_type": "code", "source": "%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0\",\n        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\"\n    }\n}", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1530680802615_0013</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:8088/proxy/application_1530680802615_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-sidspa.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:30060/node/containerlogs/container_1530680802615_0013_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'kind': 'spark', u'conf': {u'spark.jars.packages': u'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0', u'spark.jars.excludes': u'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>6</td><td>application_1530680802615_0013</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-sidspa.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:8088/proxy/application_1530680802615_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://wn1-sidspa.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:30060/node/containerlogs/container_1530680802615_0013_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "# Set variables", "cell_type": "markdown", "metadata": {}}, {"execution_count": 273, "cell_type": "code", "source": "// The Kafka broker hosts and topic used to write to Kafka\nval kafkaBrokers=\"wn0-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn2-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092\"\nval kafkaTopic=\"rawevents\"\nval zookeepers = \"zk1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181,zk3-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "zookeepers: String = zk1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181,zk3-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181"}], "metadata": {"collapsed": false}}, {"execution_count": 52, "cell_type": "code", "source": "val df1 = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"startingOffsets\", \"earliest\").\nload()\n\nval query = df1.select(from_json(col(\"key\").cast(\"Int\")), from_json(col(\"value\").cast(\"string\")) )\n.map{ case (_, value) => value.split(',') }\n.map(s => (s(0).toInt, s(1).toInt,s(2).toDouble)) \n\n", "outputs": [{"output_type": "stream", "name": "stderr", "text": "<console>:30: error: overloaded method value from_json with alternatives:\n  (e: org.apache.spark.sql.Column,schema: String,options: java.util.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.DataType)org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.DataType,options: java.util.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.StructType,options: java.util.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.DataType,options: scala.collection.immutable.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.StructType,options: scala.collection.immutable.Map[String,String])org.apache.spark.sql.Column\n cannot be applied to (org.apache.spark.sql.Column)\n       val query = df1.select(from_json(col(\"key\").cast(\"Int\")), from_json(col(\"value\").cast(\"string\")))\n                              ^\n<console>:30: error: overloaded method value from_json with alternatives:\n  (e: org.apache.spark.sql.Column,schema: String,options: java.util.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.DataType)org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.StructType)org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.DataType,options: java.util.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.StructType,options: java.util.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.DataType,options: scala.collection.immutable.Map[String,String])org.apache.spark.sql.Column <and>\n  (e: org.apache.spark.sql.Column,schema: org.apache.spark.sql.types.StructType,options: scala.collection.immutable.Map[String,String])org.apache.spark.sql.Column\n cannot be applied to (org.apache.spark.sql.Column)\n       val query = df1.select(from_json(col(\"key\").cast(\"Int\")), from_json(col(\"value\").cast(\"string\")))\n                                                                 ^\n"}], "metadata": {"collapsed": false}}, {"execution_count": 98, "cell_type": "code", "source": "%%bash\nhdfs dfs -ls /path/checkpointloc", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found 4 items\ndrwxr-xr-x   - livy supergroup          0 2018-07-04 11:32 /path/checkpointloc/commits\n-rw-r--r--   1 livy supergroup         45 2018-07-04 11:32 /path/checkpointloc/metadata\ndrwxr-xr-x   - livy supergroup          0 2018-07-04 11:33 /path/checkpointloc/offsets\ndrwxr-xr-x   - livy supergroup          0 2018-07-04 11:32 /path/checkpointloc/sources\n"}], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\n\nval df1 = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"group.id\", \"my-group-5\").\noption(\"startingOffsets\", \"earliest\").\nload()\n\n                                 \nvar query = df1.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\").as[(String,String)].\nselect(col(\"key\"),col(\"value\")).\nwriteStream.\nformat(\"console\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart()\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 255, "cell_type": "code", "source": "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\n\nval df1 = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"group.id\", \"my-group-2\").\noption(\"startingOffsets\", \"earliest\").\nload()\n\n                                 \nval query = df1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS string)\").as[(String, String)].\nselect(col(\"key\"),col(\"value\")).\nwithColumn(\"cols\",split(col(\"value\"),\",\")).\nselect(\n  $\"cols\".getItem(0).as(\"HouseHoldId\"),\n  $\"cols\".getItem(1).as(\"Timestamp\").cast(\"timestamp\"),\n  $\"cols\".getItem(2).as(\"Value\")\n).\nwithWatermark(\"Timestamp\",\"1 second\").\ngroupBy(\"Timestamp\").\ncount().\nalias(\"count\").\nselect(\"Timestamp\",\"count\").\nwriteStream.\nformat(\"console\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2dba0855"}], "metadata": {"collapsed": false}}, {"execution_count": 271, "cell_type": "code", "source": "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\n\nval df1 = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", kafkaTopic).\noption(\"enable.auto.commit\",\"false\")\noption(\"startingOffsets\", \"earliest\").\n\nload()\n\nvar query = df1.selectExpr(\"CAST(value AS STRING)\").as[(String)].\nselect(col(\"value\").cast(\"string\")).\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointloc\").\noption(\"topic\", \"cleanseevents\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart()\n", "outputs": [{"output_type": "stream", "name": "stderr", "text": "An error was encountered:\nInvalid status code '404' from http://hn0-sidspa.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:8998/sessions/5 with error payload: \"Session '5' not found.\"\n"}], "metadata": {"collapsed": false}}, {"execution_count": 127, "cell_type": "code", "source": "df1.printSchema()\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "# List Topics", "cell_type": "markdown", "metadata": {}}, {"execution_count": 260, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --list --zookeeper \"zk1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181,zk3-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181\"\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "__consumer_offsets\ncleanseevents\nrawevents\n"}], "metadata": {"collapsed": false}}, {"source": "# Create topic cleansevents", "cell_type": "markdown", "metadata": {}}, {"execution_count": 46, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-topics.sh --create --replication-factor 3 --partitions 8 --topic cleanseevents --zookeeper \"zk1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181,zk3-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Created topic \"cleanseevents\".\n"}], "metadata": {"collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "var ds = df1.select(\"CAST(key AS Int)\",from_json(col(\"value\").cast(\"string\").split(\",\")).\n                    ", "outputs": [{"output_type": "stream", "name": "stderr", "text": "org.apache.spark.sql.AnalysisException: Queries with streaming sources must be executed with writeStream.start();;\nkafka\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.org$apache$spark$sql$catalyst$analysis$UnsupportedOperationChecker$$throwError(UnsupportedOperationChecker.scala:297)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:36)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$$anonfun$checkForBatch$1.apply(UnsupportedOperationChecker.scala:34)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n  at org.apache.spark.sql.catalyst.analysis.UnsupportedOperationChecker$.checkForBatch(UnsupportedOperationChecker.scala:34)\n  at org.apache.spark.sql.execution.QueryExecution.assertSupported(QueryExecution.scala:63)\n  at org.apache.spark.sql.execution.QueryExecution.withCachedData$lzycompute(QueryExecution.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.withCachedData(QueryExecution.scala:72)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:78)\n  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:78)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)\n  at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)\n  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2837)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2150)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2363)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:241)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:637)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:596)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:605)\n  ... 50 elided\n"}], "metadata": {"collapsed": false}}, {"execution_count": 279, "cell_type": "code", "source": "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\n\nval df1 = spark.readStream.format(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"subscribe\", \"__consumer_offsets\").\noption(\"startingOffsets\", \"earliest\").\nload()\n\nvar query = df1.selectExpr(\"CAST(value AS STRING)\").as[(String)].\nselect(col(\"value\").cast(\"string\")).\nwriteStream.\nformat(\"kafka\").\noption(\"kafka.bootstrap.servers\", kafkaBrokers).\noption(\"checkpointLocation\", \"/path/checkpointloc\").\noption(\"topic\", \"cleanseevents\").\noption(\"failOnDataLoss\",\"false\").\noutputMode(\"append\").\nstart()\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "query: org.apache.spark.sql.streaming.StreamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@65f56b13"}], "metadata": {"collapsed": false}}, {"source": "## List of consumer groups", "cell_type": "markdown", "metadata": {}}, {"execution_count": 270, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-consumer-groups.sh  --bootstrap-server \"wn0-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn2-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092\" --list ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "spark-kafka-source-4fabfbd7-07bc-4741-aef4-81a798e6e871-1330853161-driver-0\nspark-kafka-source-da2f0179-081d-4999-86c3-c7b72d15a978--1471152131-driver-0\nspark-kafka-source-781f8643-e840-4ec6-8604-f9743cbad184--1229818353-driver-0\n"}], "metadata": {"collapsed": false}}, {"source": "## Delete consumer group", "cell_type": "markdown", "metadata": {}}, {"execution_count": 142, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-consumer-groups.sh --zookeeper \"zk1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181,zk3-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:2181\" --delete --group console-consumer-95359", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Delete for group console-consumer-95359 failed because group does not exist.\n"}], "metadata": {"collapsed": false}}, {"source": "## describe group", "cell_type": "markdown", "metadata": {"collapsed": false}}, {"execution_count": 268, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-consumer-groups.sh  --bootstrap-server \"wn0-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn2-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092\" --describe  --group  \"spark-kafka-source-4fabfbd7-07bc-4741-aef4-81a798e6e871-1330853161-driver-0\"", "outputs": [{"output_type": "stream", "name": "stdout", "text": "GROUP                          TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             OWNER\nspark-kafka-source-4fabfbd7-07bc-4741-aef4-81a798e6e871-1330853161-driver-0 rawevents                      0          unknown         24812354        unknown         consumer-71_/10.0.0.22\n"}], "metadata": {"collapsed": false}}, {"execution_count": 156, "cell_type": "code", "source": "%%bash\n/usr/hdp/current/kafka-broker/bin/kafka-consumer-groups.sh  --bootstrap-server \"wn0-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn1-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092,wn2-kafka.vnl0vzclrykufj5kxd5qbv4pse.cx.internal.cloudapp.net:9092\" --create --group \"raweventreader\"", "outputs": [{"output_type": "stream", "name": "stderr", "text": "Exception in thread \"main\" joptsimple.UnrecognizedOptionException: create is not a recognized option\n\tat joptsimple.OptionException.unrecognizedOption(OptionException.java:108)\n\tat joptsimple.OptionParser.handleLongOptionToken(OptionParser.java:449)\n\tat joptsimple.OptionParserState$2.handleArgument(OptionParserState.java:56)\n\tat joptsimple.OptionParser.parse(OptionParser.java:381)\n\tat kafka.admin.ConsumerGroupCommand$ConsumerGroupCommandOptions.<init>(ConsumerGroupCommand.scala:446)\n\tat kafka.admin.ConsumerGroupCommand$.main(ConsumerGroupCommand.scala:46)\n\tat kafka.admin.ConsumerGroupCommand.main(ConsumerGroupCommand.scala)\n"}], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}