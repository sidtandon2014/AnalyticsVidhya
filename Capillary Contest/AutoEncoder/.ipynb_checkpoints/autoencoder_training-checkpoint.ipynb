{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sitandon\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4194816   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               32896     \n",
      "=================================================================\n",
      "Total params: 4,489,216\n",
      "Trainable params: 4,489,216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8192)              4202496   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 128, 128, 3)       867       \n",
      "=================================================================\n",
      "Total params: 4,644,739\n",
      "Trainable params: 4,644,739\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras as k\n",
    "from keras.layers import merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Activation\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import rotate as rot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def my_autoencode(img_shape, code_size=32):\n",
    "    H,W,C = img_shape\n",
    "    \n",
    "    # encoder\n",
    "    encoder = k.models.Sequential()\n",
    "    encoder.add(k.layers.InputLayer(img_shape))\n",
    "    encoder.add(k.layers.Conv2D(32, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Conv2D(64, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.MaxPooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Conv2D(128, (3, 3), activation='elu', padding='same'))\n",
    "    encoder.add(k.layers.AveragePooling2D((2, 2), padding='same'))\n",
    "    encoder.add(k.layers.Flatten())\n",
    "    encoder.add(k.layers.Dense(512, activation='elu'))\n",
    "    encoder.add(k.layers.Dense(256, activation='elu'))\n",
    "    encoder.add(k.layers.Dense(code_size, activation='elu'))\n",
    "    encoder.summary()\n",
    "\n",
    "    # decoder\n",
    "    decoder = k.models.Sequential()\n",
    "    decoder.add(k.layers.InputLayer((code_size,)))\n",
    "    decoder.add(k.layers.Dense(256, activation='elu'))\n",
    "    decoder.add(k.layers.Dense(512, activation='elu'))\n",
    "    decoder.add(k.layers.Dense(8192, activation='elu'))\n",
    "    decoder.add(k.layers.Reshape((8, 8, 128)))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(128, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(64, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(64, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.UpSampling2D((2, 2)))\n",
    "    decoder.add(k.layers.Conv2DTranspose(32, kernel_size=(3, 3), activation='elu', padding='same'))\n",
    "    decoder.add(k.layers.Conv2DTranspose(3, kernel_size=(3, 3), activation='elu', padding='same')) # Unsure about this\n",
    "    decoder.summary()\n",
    "    \n",
    "    return encoder, decoder\n",
    "\n",
    "all_data = np.load('images_dataset.npy')\n",
    "X_train = all_data[:2723, :]\n",
    "X_test = all_data[2723:, ]\n",
    "shape = X_train[0].shape # Get from dataset\n",
    "encoder, decoder = my_autoencode(shape, code_size=128)\n",
    "inp = k.layers.Input(shape)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "autoencoder = k.models.Model(inputs=inp, outputs=reconstruction)\n",
    "autoencoder.compile(optimizer=\"adamax\", loss='mse')\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('hackathon_autoencoder.{epoch:02d}-{val_loss:.2f}.h5', verbose=1, save_best_only=True)\n",
    "]\n",
    "\n",
    "\n",
    "# If you want to resume from a checkpoint\n",
    "#     import keras.backend as K\n",
    "#     def reset_tf_session():\n",
    "#         K.clear_session()\n",
    "#         tf.reset_default_graph()\n",
    "#         s = K.get_session()\n",
    "#         return s\n",
    "#     #### uncomment below to continue training from model checkpoint\n",
    "#     #### every time epoch counter starts at 0, so you need to track epochs manually\n",
    "#     from keras.models import load_model\n",
    "#     s = reset_tf_session()\n",
    "#     autoencoder = load_model(\"checkpoints/hackathon_autoencoder.78-508.84.h5\")  # continue after epoch 0+1\n",
    "#     encoder = autoencoder.layers[1]\n",
    "#     decoder = autoencoder.layers[2]\n",
    "\n",
    "# # Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2723 samples, validate on 303 samples\n",
      "Epoch 1/200\n",
      "2723/2723 [==============================] - 40s 15ms/step - loss: 7217.8175 - val_loss: 3020.7841\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3020.78407, saving model to hackathon_autoencoder.01-3020.78.h5\n",
      "Epoch 2/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 2188.1803 - val_loss: 2433.5588\n",
      "\n",
      "Epoch 00002: val_loss improved from 3020.78407 to 2433.55877, saving model to hackathon_autoencoder.02-2433.56.h5\n",
      "Epoch 3/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 1618.7508 - val_loss: 1418.9019\n",
      "\n",
      "Epoch 00003: val_loss improved from 2433.55877 to 1418.90192, saving model to hackathon_autoencoder.03-1418.90.h5\n",
      "Epoch 4/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 1335.6574 - val_loss: 1262.6507\n",
      "\n",
      "Epoch 00004: val_loss improved from 1418.90192 to 1262.65073, saving model to hackathon_autoencoder.04-1262.65.h5\n",
      "Epoch 5/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 1195.5051 - val_loss: 1092.9909\n",
      "\n",
      "Epoch 00005: val_loss improved from 1262.65073 to 1092.99090, saving model to hackathon_autoencoder.05-1092.99.h5\n",
      "Epoch 6/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 1116.8857 - val_loss: 1065.3215\n",
      "\n",
      "Epoch 00006: val_loss improved from 1092.99090 to 1065.32146, saving model to hackathon_autoencoder.06-1065.32.h5\n",
      "Epoch 7/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 1039.1215 - val_loss: 971.9688\n",
      "\n",
      "Epoch 00007: val_loss improved from 1065.32146 to 971.96878, saving model to hackathon_autoencoder.07-971.97.h5\n",
      "Epoch 8/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 967.8362 - val_loss: 921.5747\n",
      "\n",
      "Epoch 00008: val_loss improved from 971.96878 to 921.57467, saving model to hackathon_autoencoder.08-921.57.h5\n",
      "Epoch 9/200\n",
      "2723/2723 [==============================] - 28s 10ms/step - loss: 903.3688 - val_loss: 871.4148\n",
      "\n",
      "Epoch 00009: val_loss improved from 921.57467 to 871.41476, saving model to hackathon_autoencoder.09-871.41.h5\n",
      "Epoch 10/200\n",
      "2723/2723 [==============================] - 30s 11ms/step - loss: 859.6076 - val_loss: 864.8999\n",
      "\n",
      "Epoch 00010: val_loss improved from 871.41476 to 864.89994, saving model to hackathon_autoencoder.10-864.90.h5\n",
      "Epoch 11/200\n",
      "2723/2723 [==============================] - 31s 11ms/step - loss: 822.7519 - val_loss: 798.7842\n",
      "\n",
      "Epoch 00011: val_loss improved from 864.89994 to 798.78418, saving model to hackathon_autoencoder.11-798.78.h5\n",
      "Epoch 12/200\n",
      "2723/2723 [==============================] - 31s 11ms/step - loss: 790.8885 - val_loss: 795.5343\n",
      "\n",
      "Epoch 00012: val_loss improved from 798.78418 to 795.53435, saving model to hackathon_autoencoder.12-795.53.h5\n",
      "Epoch 13/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 767.9802 - val_loss: 790.5486\n",
      "\n",
      "Epoch 00013: val_loss improved from 795.53435 to 790.54855, saving model to hackathon_autoencoder.13-790.55.h5\n",
      "Epoch 14/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 744.0849 - val_loss: 754.2865\n",
      "\n",
      "Epoch 00014: val_loss improved from 790.54855 to 754.28648, saving model to hackathon_autoencoder.14-754.29.h5\n",
      "Epoch 15/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 701.2204 - val_loss: 720.8896\n",
      "\n",
      "Epoch 00015: val_loss improved from 754.28648 to 720.88963, saving model to hackathon_autoencoder.15-720.89.h5\n",
      "Epoch 16/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 680.1830 - val_loss: 780.6798\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 720.88963\n",
      "Epoch 17/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 664.2637 - val_loss: 687.2187\n",
      "\n",
      "Epoch 00017: val_loss improved from 720.88963 to 687.21874, saving model to hackathon_autoencoder.17-687.22.h5\n",
      "Epoch 18/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 639.9058 - val_loss: 689.0868\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 687.21874\n",
      "Epoch 19/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 626.9453 - val_loss: 707.5637\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 687.21874\n",
      "Epoch 20/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 618.1451 - val_loss: 651.5667\n",
      "\n",
      "Epoch 00020: val_loss improved from 687.21874 to 651.56665, saving model to hackathon_autoencoder.20-651.57.h5\n",
      "Epoch 21/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 597.5348 - val_loss: 667.9269\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 651.56665\n",
      "Epoch 22/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 578.6666 - val_loss: 663.5287\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 651.56665\n",
      "Epoch 23/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 568.5516 - val_loss: 640.3450\n",
      "\n",
      "Epoch 00023: val_loss improved from 651.56665 to 640.34502, saving model to hackathon_autoencoder.23-640.35.h5\n",
      "Epoch 24/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 554.6824 - val_loss: 682.8027\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 640.34502\n",
      "Epoch 25/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 542.6171 - val_loss: 642.1273\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 640.34502\n",
      "Epoch 26/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 544.7642 - val_loss: 634.9886\n",
      "\n",
      "Epoch 00026: val_loss improved from 640.34502 to 634.98863, saving model to hackathon_autoencoder.26-634.99.h5\n",
      "Epoch 27/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 522.3579 - val_loss: 642.1840\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 634.98863\n",
      "Epoch 28/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 521.0641 - val_loss: 618.1449\n",
      "\n",
      "Epoch 00028: val_loss improved from 634.98863 to 618.14488, saving model to hackathon_autoencoder.28-618.14.h5\n",
      "Epoch 29/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 501.9343 - val_loss: 656.3222\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 618.14488\n",
      "Epoch 30/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 500.0124 - val_loss: 627.4157\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 618.14488\n",
      "Epoch 31/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 494.1715 - val_loss: 642.9087\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 618.14488\n",
      "Epoch 32/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 485.7088 - val_loss: 582.9928\n",
      "\n",
      "Epoch 00032: val_loss improved from 618.14488 to 582.99283, saving model to hackathon_autoencoder.32-582.99.h5\n",
      "Epoch 33/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 473.5169 - val_loss: 613.5392\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 582.99283\n",
      "Epoch 34/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 473.9422 - val_loss: 573.4203\n",
      "\n",
      "Epoch 00034: val_loss improved from 582.99283 to 573.42027, saving model to hackathon_autoencoder.34-573.42.h5\n",
      "Epoch 35/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 455.5672 - val_loss: 578.1396\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 573.42027\n",
      "Epoch 36/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 460.3843 - val_loss: 590.2074\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 573.42027\n",
      "Epoch 37/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 451.5411 - val_loss: 599.9907\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 573.42027\n",
      "Epoch 38/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 443.2830 - val_loss: 591.6713\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 573.42027\n",
      "Epoch 39/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 446.1143 - val_loss: 579.3318\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 573.42027\n",
      "Epoch 40/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 432.9792 - val_loss: 569.6797\n",
      "\n",
      "Epoch 00040: val_loss improved from 573.42027 to 569.67974, saving model to hackathon_autoencoder.40-569.68.h5\n",
      "Epoch 41/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 432.9060 - val_loss: 623.9918\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 569.67974\n",
      "Epoch 42/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723/2723 [==============================] - 32s 12ms/step - loss: 429.9614 - val_loss: 559.9329\n",
      "\n",
      "Epoch 00042: val_loss improved from 569.67974 to 559.93290, saving model to hackathon_autoencoder.42-559.93.h5\n",
      "Epoch 43/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 402.0951 - val_loss: 560.9839\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 559.93290\n",
      "Epoch 44/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 396.3365 - val_loss: 535.8517\n",
      "\n",
      "Epoch 00044: val_loss improved from 559.93290 to 535.85172, saving model to hackathon_autoencoder.44-535.85.h5\n",
      "Epoch 45/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 392.4922 - val_loss: 584.5097\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 535.85172\n",
      "Epoch 46/200\n",
      "2723/2723 [==============================] - 32s 12ms/step - loss: 404.7217 - val_loss: 550.5380\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 535.85172\n",
      "Epoch 47/200\n",
      "2723/2723 [==============================] - 33s 12ms/step - loss: 379.5560 - val_loss: 537.4261\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 535.85172\n",
      "Epoch 48/200\n",
      "2723/2723 [==============================] - 33s 12ms/step - loss: 385.6219 - val_loss: 538.9961\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 535.85172\n",
      "Epoch 49/200\n",
      "2723/2723 [==============================] - 33s 12ms/step - loss: 372.6206 - val_loss: 559.3529\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 535.85172\n",
      "Epoch 50/200\n",
      "2723/2723 [==============================] - 34s 13ms/step - loss: 375.3638 - val_loss: 538.9288\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 535.85172\n",
      "Epoch 51/200\n",
      "2723/2723 [==============================] - 34s 13ms/step - loss: 367.4669 - val_loss: 541.6905\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 535.85172\n",
      "Epoch 52/200\n",
      "2723/2723 [==============================] - 34s 13ms/step - loss: 372.0211 - val_loss: 552.2077\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 535.85172\n",
      "Epoch 53/200\n",
      "2723/2723 [==============================] - 34s 13ms/step - loss: 364.1725 - val_loss: 560.4943\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 535.85172\n",
      "Epoch 54/200\n",
      "2723/2723 [==============================] - 34s 13ms/step - loss: 365.8459 - val_loss: 632.4245\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 535.85172\n",
      "Epoch 55/200\n",
      "2723/2723 [==============================] - 35s 13ms/step - loss: 361.3872 - val_loss: 526.2121\n",
      "\n",
      "Epoch 00055: val_loss improved from 535.85172 to 526.21212, saving model to hackathon_autoencoder.55-526.21.h5\n",
      "Epoch 56/200\n",
      "2723/2723 [==============================] - 35s 13ms/step - loss: 345.9238 - val_loss: 527.5005\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 526.21212\n",
      "Epoch 57/200\n",
      "2723/2723 [==============================] - 35s 13ms/step - loss: 339.5636 - val_loss: 550.0813\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 526.21212\n",
      "Epoch 58/200\n",
      "2723/2723 [==============================] - 35s 13ms/step - loss: 345.3768 - val_loss: 545.9494\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 526.21212\n",
      "Epoch 59/200\n",
      "2723/2723 [==============================] - 35s 13ms/step - loss: 340.5526 - val_loss: 523.5182\n",
      "\n",
      "Epoch 00059: val_loss improved from 526.21212 to 523.51820, saving model to hackathon_autoencoder.59-523.52.h5\n",
      "Epoch 60/200\n",
      "2723/2723 [==============================] - 37s 13ms/step - loss: 335.8630 - val_loss: 542.8759\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 523.51820\n",
      "Epoch 61/200\n",
      "2723/2723 [==============================] - 37s 13ms/step - loss: 334.5333 - val_loss: 562.8657\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 523.51820\n",
      "Epoch 62/200\n",
      "2723/2723 [==============================] - 36s 13ms/step - loss: 344.1129 - val_loss: 574.9466\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 523.51820\n",
      "Epoch 63/200\n",
      "2723/2723 [==============================] - 36s 13ms/step - loss: 333.8640 - val_loss: 538.6939\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 523.51820\n",
      "Epoch 64/200\n",
      "2723/2723 [==============================] - 37s 13ms/step - loss: 320.9872 - val_loss: 519.3189\n",
      "\n",
      "Epoch 00064: val_loss improved from 523.51820 to 519.31886, saving model to hackathon_autoencoder.64-519.32.h5\n",
      "Epoch 65/200\n",
      "2723/2723 [==============================] - 37s 13ms/step - loss: 318.1904 - val_loss: 542.1224\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 519.31886\n",
      "Epoch 66/200\n",
      "2723/2723 [==============================] - 36s 13ms/step - loss: 310.9828 - val_loss: 526.3154\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 519.31886\n",
      "Epoch 67/200\n",
      "2723/2723 [==============================] - 36s 13ms/step - loss: 307.0522 - val_loss: 565.9680\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 519.31886\n",
      "Epoch 68/200\n",
      "2723/2723 [==============================] - 36s 13ms/step - loss: 308.6121 - val_loss: 519.4956\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 519.31886\n",
      "Epoch 69/200\n",
      "2723/2723 [==============================] - 36s 13ms/step - loss: 303.3493 - val_loss: 532.9964\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 519.31886\n",
      "Epoch 70/200\n",
      "2723/2723 [==============================] - 38s 14ms/step - loss: 298.0283 - val_loss: 519.5435\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 519.31886\n",
      "Epoch 71/200\n",
      "2723/2723 [==============================] - 39s 14ms/step - loss: 297.1426 - val_loss: 542.7620\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 519.31886\n",
      "Epoch 72/200\n",
      "2723/2723 [==============================] - 39s 14ms/step - loss: 292.4453 - val_loss: 522.3734\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 519.31886\n",
      "Epoch 73/200\n",
      "2723/2723 [==============================] - 39s 14ms/step - loss: 290.4591 - val_loss: 563.0164\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 519.31886\n",
      "Epoch 74/200\n",
      "2723/2723 [==============================] - 41s 15ms/step - loss: 290.6407 - val_loss: 539.1007\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 519.31886\n",
      "Epoch 75/200\n",
      "2723/2723 [==============================] - 41s 15ms/step - loss: 291.4643 - val_loss: 532.2677\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 519.31886\n",
      "Epoch 76/200\n",
      "2723/2723 [==============================] - 41s 15ms/step - loss: 276.6441 - val_loss: 526.2524\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 519.31886\n",
      "Epoch 77/200\n",
      "2723/2723 [==============================] - 41s 15ms/step - loss: 275.4117 - val_loss: 520.5095\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 519.31886\n",
      "Epoch 78/200\n",
      "2723/2723 [==============================] - 41s 15ms/step - loss: 267.4507 - val_loss: 542.3206\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 519.31886\n",
      "Epoch 79/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 265.6353 - val_loss: 525.0150\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 519.31886\n",
      "Epoch 80/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 269.4994 - val_loss: 553.7761\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 519.31886\n",
      "Epoch 81/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 267.3951 - val_loss: 543.0591\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 519.31886\n",
      "Epoch 82/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 271.0532 - val_loss: 523.8911\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 519.31886\n",
      "Epoch 83/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 260.8841 - val_loss: 533.7174\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 519.31886\n",
      "Epoch 84/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 253.2493 - val_loss: 559.8943\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 519.31886\n",
      "Epoch 85/200\n",
      "2723/2723 [==============================] - 43s 16ms/step - loss: 250.6014 - val_loss: 535.1202\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 519.31886\n",
      "Epoch 86/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 244.8311 - val_loss: 555.4643\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 519.31886\n",
      "Epoch 87/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 251.1381 - val_loss: 541.7903\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 519.31886\n",
      "Epoch 88/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 246.3781 - val_loss: 536.4352\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 519.31886\n",
      "Epoch 89/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 253.5018 - val_loss: 533.3545\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 519.31886\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723/2723 [==============================] - 44s 16ms/step - loss: 247.6757 - val_loss: 527.6314\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 519.31886\n",
      "Epoch 91/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 242.3918 - val_loss: 552.0657\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 519.31886\n",
      "Epoch 92/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 232.5682 - val_loss: 544.5623\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 519.31886\n",
      "Epoch 93/200\n",
      "2723/2723 [==============================] - 44s 16ms/step - loss: 234.7119 - val_loss: 550.3197\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 519.31886\n",
      "Epoch 94/200\n",
      "2723/2723 [==============================] - 46s 17ms/step - loss: 239.2253 - val_loss: 536.5065\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 519.31886\n",
      "Epoch 95/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 233.3875 - val_loss: 549.7968\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 519.31886\n",
      "Epoch 96/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 225.4591 - val_loss: 548.4336\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 519.31886\n",
      "Epoch 97/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 228.6052 - val_loss: 578.1969\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 519.31886\n",
      "Epoch 98/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 225.6995 - val_loss: 545.6824\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 519.31886\n",
      "Epoch 99/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 234.4369 - val_loss: 533.4477\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 519.31886\n",
      "Epoch 100/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 224.2146 - val_loss: 540.0759\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 519.31886\n",
      "Epoch 101/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 216.1605 - val_loss: 556.1206\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 519.31886\n",
      "Epoch 102/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 218.4626 - val_loss: 534.8406\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 519.31886\n",
      "Epoch 103/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 216.2797 - val_loss: 536.6329\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 519.31886\n",
      "Epoch 104/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 214.9018 - val_loss: 542.0835\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 519.31886\n",
      "Epoch 105/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 215.4111 - val_loss: 553.2521\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 519.31886\n",
      "Epoch 106/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 217.9383 - val_loss: 556.3623\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 519.31886\n",
      "Epoch 107/200\n",
      "2723/2723 [==============================] - 47s 17ms/step - loss: 211.1902 - val_loss: 556.8396\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 519.31886\n",
      "Epoch 108/200\n",
      "2723/2723 [==============================] - 59s 22ms/step - loss: 204.9568 - val_loss: 535.0731\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 519.31886\n",
      "Epoch 109/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 203.0918 - val_loss: 543.6970\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 519.31886\n",
      "Epoch 110/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 204.1542 - val_loss: 561.7152\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 519.31886\n",
      "Epoch 111/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 207.7789 - val_loss: 563.3560\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 519.31886\n",
      "Epoch 112/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 208.9510 - val_loss: 541.0167\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 519.31886\n",
      "Epoch 113/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 196.1158 - val_loss: 544.4089\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 519.31886\n",
      "Epoch 114/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 196.8122 - val_loss: 564.4367\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 519.31886\n",
      "Epoch 115/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 196.4870 - val_loss: 541.2203\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 519.31886\n",
      "Epoch 116/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 195.3887 - val_loss: 546.8175\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 519.31886\n",
      "Epoch 117/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 198.3842 - val_loss: 562.2602\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 519.31886\n",
      "Epoch 118/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 192.9697 - val_loss: 553.7500\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 519.31886\n",
      "Epoch 119/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 190.0505 - val_loss: 557.2722\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 519.31886\n",
      "Epoch 120/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 196.6101 - val_loss: 555.9527\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 519.31886\n",
      "Epoch 121/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 193.9579 - val_loss: 553.9443\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 519.31886\n",
      "Epoch 122/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 189.1787 - val_loss: 558.2426\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 519.31886\n",
      "Epoch 123/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 188.0527 - val_loss: 557.0396\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 519.31886\n",
      "Epoch 124/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 188.5540 - val_loss: 567.6262\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 519.31886\n",
      "Epoch 125/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 188.1323 - val_loss: 565.3075\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 519.31886\n",
      "Epoch 126/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 184.4189 - val_loss: 568.9542\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 519.31886\n",
      "Epoch 127/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 183.6932 - val_loss: 566.1821\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 519.31886\n",
      "Epoch 128/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 184.9559 - val_loss: 592.2890\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 519.31886\n",
      "Epoch 129/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 193.5099 - val_loss: 575.2292\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 519.31886\n",
      "Epoch 130/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 183.8507 - val_loss: 561.9260\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 519.31886\n",
      "Epoch 131/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 173.8921 - val_loss: 574.3060\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 519.31886\n",
      "Epoch 132/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 173.7598 - val_loss: 555.6342\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 519.31886\n",
      "Epoch 133/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 178.4052 - val_loss: 569.2902\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 519.31886\n",
      "Epoch 134/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 181.0016 - val_loss: 598.8109\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 519.31886\n",
      "Epoch 135/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 178.2449 - val_loss: 570.2523\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 519.31886\n",
      "Epoch 136/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 174.3306 - val_loss: 592.4169\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 519.31886\n",
      "Epoch 137/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 173.1606 - val_loss: 558.3110\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 519.31886\n",
      "Epoch 138/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 174.3218 - val_loss: 559.1171\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 519.31886\n",
      "Epoch 139/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 170.0431 - val_loss: 561.9524\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 519.31886\n",
      "Epoch 140/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723/2723 [==============================] - 65s 24ms/step - loss: 174.5407 - val_loss: 569.9338\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 519.31886\n",
      "Epoch 141/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 172.4167 - val_loss: 556.3219\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 519.31886\n",
      "Epoch 142/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 169.3352 - val_loss: 573.9597\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 519.31886\n",
      "Epoch 143/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 172.8431 - val_loss: 573.7021\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 519.31886\n",
      "Epoch 144/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 174.3868 - val_loss: 568.8549\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 519.31886\n",
      "Epoch 145/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 168.0164 - val_loss: 564.0805\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 519.31886\n",
      "Epoch 146/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 161.1612 - val_loss: 570.2614\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 519.31886\n",
      "Epoch 147/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 162.7289 - val_loss: 600.8569\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 519.31886\n",
      "Epoch 148/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 167.7529 - val_loss: 572.6353\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 519.31886\n",
      "Epoch 149/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 162.5517 - val_loss: 585.1904\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 519.31886\n",
      "Epoch 150/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 162.4512 - val_loss: 573.6565\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 519.31886\n",
      "Epoch 151/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 158.8892 - val_loss: 572.7147\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 519.31886\n",
      "Epoch 152/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 161.0256 - val_loss: 570.0925\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 519.31886\n",
      "Epoch 153/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 160.3964 - val_loss: 577.5203\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 519.31886\n",
      "Epoch 154/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 164.4014 - val_loss: 569.3162\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 519.31886\n",
      "Epoch 155/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 161.4822 - val_loss: 583.4377\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 519.31886\n",
      "Epoch 156/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 160.1070 - val_loss: 603.1346\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 519.31886\n",
      "Epoch 157/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 157.0980 - val_loss: 578.4188\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 519.31886\n",
      "Epoch 158/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 156.4895 - val_loss: 570.5155\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 519.31886\n",
      "Epoch 159/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 154.7067 - val_loss: 580.8728\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 519.31886\n",
      "Epoch 160/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 161.0223 - val_loss: 568.7851\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 519.31886\n",
      "Epoch 161/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 154.5281 - val_loss: 580.1267\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 519.31886\n",
      "Epoch 162/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 155.2597 - val_loss: 585.5717\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 519.31886\n",
      "Epoch 163/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 156.8247 - val_loss: 587.1581\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 519.31886\n",
      "Epoch 164/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 158.0406 - val_loss: 567.5936\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 519.31886\n",
      "Epoch 165/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 152.8363 - val_loss: 581.9466\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 519.31886\n",
      "Epoch 166/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 151.1529 - val_loss: 573.4665\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 519.31886\n",
      "Epoch 167/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 148.3979 - val_loss: 587.2517\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 519.31886\n",
      "Epoch 168/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 153.8282 - val_loss: 583.4990\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 519.31886\n",
      "Epoch 169/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 152.4422 - val_loss: 580.4998\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 519.31886\n",
      "Epoch 170/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 148.7904 - val_loss: 579.3149\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 519.31886\n",
      "Epoch 171/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 148.3624 - val_loss: 595.9775\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 519.31886\n",
      "Epoch 172/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 148.8041 - val_loss: 586.0858\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 519.31886\n",
      "Epoch 173/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 145.4154 - val_loss: 573.4401\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 519.31886\n",
      "Epoch 174/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 146.9091 - val_loss: 600.9406\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 519.31886\n",
      "Epoch 175/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 147.2881 - val_loss: 597.0295\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 519.31886\n",
      "Epoch 176/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 146.1918 - val_loss: 576.7467\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 519.31886\n",
      "Epoch 177/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 145.3169 - val_loss: 584.3476\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 519.31886\n",
      "Epoch 178/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 145.4641 - val_loss: 582.8081\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 519.31886\n",
      "Epoch 179/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 147.4346 - val_loss: 581.6754\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 519.31886\n",
      "Epoch 180/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 147.4701 - val_loss: 588.9322\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 519.31886\n",
      "Epoch 181/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 148.7118 - val_loss: 584.5532\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 519.31886\n",
      "Epoch 182/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 144.1831 - val_loss: 582.8209\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 519.31886\n",
      "Epoch 183/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 138.4814 - val_loss: 588.0124\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 519.31886\n",
      "Epoch 184/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 140.5137 - val_loss: 587.3303\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 519.31886\n",
      "Epoch 185/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 142.6703 - val_loss: 584.4185\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 519.31886\n",
      "Epoch 186/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 146.1933 - val_loss: 592.1005\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 519.31886\n",
      "Epoch 187/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 143.8752 - val_loss: 596.4261\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 519.31886\n",
      "Epoch 188/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 138.8404 - val_loss: 582.0320\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 519.31886\n",
      "Epoch 189/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 135.5285 - val_loss: 584.5624\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 519.31886\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2723/2723 [==============================] - 64s 24ms/step - loss: 136.4714 - val_loss: 583.9222\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 519.31886\n",
      "Epoch 191/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 136.9461 - val_loss: 611.1627\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 519.31886\n",
      "Epoch 192/200\n",
      "2723/2723 [==============================] - 65s 24ms/step - loss: 139.5341 - val_loss: 582.4551\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 519.31886\n",
      "Epoch 193/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 140.3546 - val_loss: 589.0793\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 519.31886\n",
      "Epoch 194/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 137.2948 - val_loss: 583.8648\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 519.31886\n",
      "Epoch 195/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 139.5748 - val_loss: 603.6452\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 519.31886\n",
      "Epoch 196/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 141.1964 - val_loss: 589.3944\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 519.31886\n",
      "Epoch 197/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 135.8030 - val_loss: 588.8798\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 519.31886\n",
      "Epoch 198/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 135.2611 - val_loss: 594.1352\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 519.31886\n",
      "Epoch 199/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 136.8887 - val_loss: 582.5212\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 519.31886\n",
      "Epoch 200/200\n",
      "2723/2723 [==============================] - 64s 24ms/step - loss: 134.9929 - val_loss: 597.8678\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 519.31886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x237cf28b518>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x=X_train, y=X_train,\n",
    "                validation_data=[X_test, X_test],\n",
    "                epochs=200,\n",
    "                batch_size=32,\n",
    "                shuffle=True,\n",
    "                callbacks = callbacks\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
